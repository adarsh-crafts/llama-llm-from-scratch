{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5428b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Coding\\Python\\llama-llm-from-scratch\\venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aec6b3",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67c298",
   "metadata": {},
   "source": [
    "*   intermediate size is the embedding space where the model learns more complex relationships and it is often a integer rounded of to `2.68 * hidden_size`.  \n",
    "\n",
    "*   We must make sure it is a multiple of 32 as modern GPUs are very efficieint in performing calculations on matrices of the sizes 2, 4, 8, 16, 32, 64, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2cebf7",
   "metadata": {},
   "source": [
    "##### Layer Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e1da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128  # dimesionality of the model's hidden state\n",
    "ffn_intermediate_ratio = 8/3 # aprox 2.68\n",
    "multiple_of = 32  # to make sure the intermediate size is a multiple of this value\n",
    "intermediate_size = int(hidden_size * ffn_intermediate_ratio)\n",
    "\n",
    "# making sure \"hidden_size\" is a multiple of \"multiple_of\"\n",
    "intermediate_size = ((intermediate_size + multiple_of - 1) // multiple_of) * multiple_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9cb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_act = 'silu'  # activation function\n",
    "rms_norm_eps = 1e-5\n",
    "ffn_bias = False  # whether or not to use bias in FFN linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabea42",
   "metadata": {},
   "source": [
    "##### Sample Input\n",
    "*(a.k.a output of attention mechanism)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b648a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "input_to_ffn_block = torch.randn(batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2c4acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "    hidden_size: 128\n",
      "    intermediate_size: 352 (Calculated from ratio 2.67, multiple of 32)\n",
      "    hidden_act: silu\n",
      "    rms_norm_eps: 1e-05\n",
      "\n",
      "Sample Input Shape (Before FFN Block Norm):\n",
      "    input_to_ffn_block: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print('Configuration:')\n",
    "print(f'    hidden_size: {hidden_size}')\n",
    "print(f\"    intermediate_size: {intermediate_size} (Calculated from ratio {ffn_intermediate_ratio:.2f}, multiple of {multiple_of})\")\n",
    "print(f\"    hidden_act: {hidden_act}\")\n",
    "print(f\"    rms_norm_eps: {rms_norm_eps}\")\n",
    "\n",
    "print(\"\\nSample Input Shape (Before FFN Block Norm):\")\n",
    "print(f\"    input_to_ffn_block: {input_to_ffn_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a23b7",
   "metadata": {},
   "source": [
    "# Pre-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0720187",
   "metadata": {},
   "source": [
    "Unlike transformers that apply LayerNorm *after* the FFN and residual connection, Llama uses a pre-normalization aproach (`post-attention normalization` in the original `Llama4TextDecoderLayer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779993e",
   "metadata": {},
   "source": [
    "##### EXPLANATION\n",
    "Before the FFN, we apply Root Mean Square Normalization (RMSNorm) to stabilize the training process.\n",
    "\n",
    "The problem with deep networks is that the numbers flowing through them can get too big or small, which makes learning difficult. Normalization fixes this, but it also affects the vector's magnitude (its length or signal strength). The original magnitude might have been important, so we need a way to let the model recover it if needed.\n",
    "\n",
    "RMSNorm solves this with a two-step process:\n",
    "\n",
    "Normalize: First, the input vector is scaled to a standard size. This stabilizes the numbers but changes the original magnitude.\n",
    "\n",
    "Rescale: Second, the normalized vector is multiplied by a learnable weight. This acts like a \"volume knob\" that allows the model to learn how to scale the signal back up or down, effectively learning to restore the magnitude if the original strength was meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1e3c1",
   "metadata": {},
   "source": [
    "`self.weight`: The RMSNorm calculation first normalizes the hidden state to have a unit standard deviation. This line then multiplies that normalized state by the weight tensor.  \n",
    "( *During learning, the model learns to scale them up or downas needed to improve performance.* )  \n",
    "\n",
    "\n",
    "`variance = hidden_states.pow(2).mean(-1, keepdim=True)`  \n",
    "example:  \n",
    "[ [1, 2, 3],  \n",
    "&nbsp;&nbsp; [4, 5, 6] ]  \n",
    "\n",
    "*   `-1` tells to calc mean along the cols:  \n",
    "    *   Mean of the 1st row [1, 2, 3] is 2  \n",
    "    *   Mean of the 2nd row [4, 5, 6] is 5.  \n",
    "\n",
    "*   `keepdim`:\n",
    "    *   `If keepdim=False (the default)`:\n",
    "The dimension you averaged over is removed. The output shape would be a 1D tensor: [2, 5]\n",
    "\n",
    "    *   `If keepdim=True`:\n",
    "The dimension is kept, but its size becomes 1 (*we collapsed 3 number into 1 number*). The output shape would be a 2D tensor: [[2], [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160e3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # learnable gain parameter\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)  # float32 for stability\n",
    "\n",
    "        # calculate variance (mean of square)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # normalise: input / sqrt(variance + epsilon)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "\n",
    "        # apply the learnable weights andcast back to the original dtype\n",
    "        return (self.weight * hidden_states).to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ea2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attention_norm = SimplifiedRMSNorm(hidden_size, eps = rms_norm_eps)\n",
    "normalized_hidden_states = post_attention_norm(input_to_ffn_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4550908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Post-Attention RMSNorm:\n",
      "    normalized_hidden-states: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape after Post-Attention RMSNorm:\")\n",
    "print(f'    normalized_hidden-states: {normalized_hidden_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf72d8e",
   "metadata": {},
   "source": [
    "# Feed-Forward Network \n",
    "(MLP with Gated linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecc477",
   "metadata": {},
   "source": [
    "##### Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1cb6c",
   "metadata": {},
   "source": [
    "The core of the LLM's dense layers is an MLPusing a gated mechanism, SiLU Gated Linear Unit (SwiGLU).  \n",
    "It consists of three layers:\n",
    "1.  **`gate_proj`:** Projects the input to the `intermediate_size`.\n",
    "2.  **`up_proj`:** Also projects the input to the `intermediate_size`.\n",
    "3.  **`down_proj`:** Projects the result back down to the `hidden_size`.\n",
    " \n",
    "Calculation: `down_proj( F.silu(gate_proj(x)) * up_proj(x))`\n",
    "- The `gate_proj` output is passed through an activation function (SiLU/Swish).\n",
    "- This activated gate is element-wise multiplied by the `up_proj` output.\n",
    "- The result is then projected back to the original hidden dimension by `down_proj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0787fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "up_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "down_proj = nn.Linear(intermediate_size, hidden_size, bias=ffn_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a131",
   "metadata": {},
   "source": [
    "Normally, `ACT2FN` is used here, it's like a lookup table that we can use instead of hardcoding the function `nn.SiLU`  \n",
    "\n",
    "*Ecample:*  \n",
    "*In a config file we can write:*  \n",
    "` \"hidden_act\": \"silu\"`  \n",
    "\n",
    "*In the model code, we would:*  \n",
    "`activation_function = ACT2FN[config.hidden_act]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0bafae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hidden_act == \"silu\":\n",
    "    activation_fn = nn.SiLU()\n",
    "else:\n",
    "    # we can add any other activation function or just raise error\n",
    "    raise NotImplementedError(f\"Activation {hidden_act} not implemented as of now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d23bb1",
   "metadata": {},
   "source": [
    "##### Applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151facbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_output = gate_proj(normalized_hidden_states)\n",
    "up_output = up_proj(normalized_hidden_states)\n",
    "\n",
    "# Applying the function acc to equation previously explained\n",
    "activated_gate = activation_fn(gate_output)\n",
    "gated_result = activated_gate * up_output\n",
    "ffn_output = down_proj(gated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12db7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes wihtin FFN:\n",
      "    gate_output: torch.Size([2, 10, 352])\n",
      "    up_output: torch.Size([2, 10, 352])\n",
      "\n",
      "    gated_result: torch.Size([2, 10, 352])\n",
      "    ffn_output: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print('Shapes wihtin FFN:')\n",
    "print(f'    gate_output: {gate_output.shape}')\n",
    "print(f'    up_output: {up_output.shape}')\n",
    "print()\n",
    "print(f'    gated_result: {gated_result.shape}')\n",
    "print(f'    ffn_output: {ffn_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07de71",
   "metadata": {},
   "source": [
    "# Residual Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60681610",
   "metadata": {},
   "source": [
    "When you take an iunput to a layer/block and add it to the output of the layer.\n",
    "`final_output = input + f(input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bccb9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after FFN Residual Connection: \n",
      "    final_output: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "final_output = input_to_ffn_block + ffn_output\n",
    "\n",
    "print('Shape after FFN Residual Connection: ')\n",
    "print(f'    final_output: {final_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7eca9",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768b873",
   "metadata": {},
   "source": [
    "##### Initialising class SimplifiedLlama4FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fd28d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedLlama4FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.hidden_act = config['hidden_act']\n",
    "        self.ffn_bias = config['ffn_bias']\n",
    "        self.rms_norm_eps = config['rms_norm_eps']\n",
    "\n",
    "        # normlization before MLP\n",
    "        self.norm = SimplifiedRMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n",
    "\n",
    "        # MLP layers\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.ffn_bias)\n",
    "\n",
    "        # activation function\n",
    "        if self.hidden_act == \"silu\":\n",
    "            self.activation_fn = nn.SiLU()\n",
    "        else:\n",
    "            raise NotImplementedError(f'Activation {self.hidden_act} not implemented.')\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # aplpying normalization\n",
    "        normalized_states = self.norm(hidden_states)\n",
    "\n",
    "        # applying mlp\n",
    "        gate = self.gate_proj(normalized_states)\n",
    "        up = self.up_proj(normalized_states)\n",
    "        down = self.down_proj(self.activation_fn(gate) * up)\n",
    "\n",
    "        return down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39122042",
   "metadata": {},
   "source": [
    "##### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc90a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_config_dict = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'intermediate_size': intermediate_size,\n",
    "    'hidden_act': hidden_act,\n",
    "    'ffn_bias': ffn_bias,\n",
    "    'rms_norm_eps': rms_norm_eps\n",
    "}\n",
    "\n",
    "simplified_ffn_module = SimplifiedLlama4FFN(ffn_config_dict)\n",
    "\n",
    "# forward pass using the module\n",
    "mlp_output_from_module = simplified_ffn_module(input_to_ffn_block)\n",
    "\n",
    "# applying residual connection\n",
    "final_output_from_module = input_to_ffn_block + mlp_output_from_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8a32d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape from simplified FFN module: torch.Size([2, 10, 128])\n",
      "Output shape after external residual connection:  torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print(f'Output shape from simplified FFN module: {mlp_output_from_module.shape}')\n",
    "print(f'Output shape after external residual connection: ', final_output_from_module.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d6f45",
   "metadata": {},
   "source": [
    "##### Simple element-wise verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa9cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs closely match: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Outputs closely match: {torch.allclose(final_output, final_output_from_module, atol=1e-6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe287e",
   "metadata": {},
   "source": [
    "It's `False` because the manual layers such as `post_attention_norm`, `gate_proj`, etc. and the second set of weights inside `SimplifiedLlama4FFN` are randomly loaded.\n",
    "\n",
    "*we'd have to manually load the same weights*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
