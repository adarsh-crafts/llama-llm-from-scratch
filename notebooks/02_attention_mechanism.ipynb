{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948ca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Coding\\Python\\llama-from-scratch\\venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75108c81",
   "metadata": {},
   "source": [
    "# Config settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5378bf1",
   "metadata": {},
   "source": [
    "#### Architecural Dimensions\n",
    "$\\underline{\\text{Grouped-Query Attention (GQA)}}$  \n",
    "This is a technique in which we use fewer number of Key/Value heads than the Query heads.  This method requires significantly less memory, and can generate text much faster with a very small impact on the overall accuracy.  \n",
    "> __note:__ The number of Query heads must be perfectly divisble by the number of Key/Value heads.  \n",
    "\n",
    "*In this case, we use one Key and Value heads per 4 Query heads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fa0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 128   # synonymous with embeddings dimension\n",
    "num_attention_heads = 16    # The no. of attention query heads\n",
    "num_key_value_heads = 4     # The no. of Key & Value heads [Grouped-Query Attention (GQA)]\n",
    "head_dim = hidden_size // num_attention_heads   # Dimension of each atention head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31279561",
   "metadata": {},
   "source": [
    "#### Positional Embedding Parameters\n",
    "\n",
    "Instead of traditional positional embeddings, this model uses RoPE to encode the order of tokens. RoPE modifies the Query and Key vectors using rotations, which elegantly injects relative positional information directly into the self-attention calculation.  \n",
    "\n",
    "\n",
    "$\\underline{\\text{Rotary Positional Encodings (RoPE)}}$  \n",
    "* **Relative Position:** The attention score between two tokens becomes sensitive to their relative distance, not their absolute positions.\n",
    "* **No Trainable Parameters:** Positional information is added via a deterministic function, requiring no extra parameters to be learned.\n",
    "* **Long Sequence Extrapolation:** RoPE has been shown to be effective at handling sequences longer than the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3aea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positional_embeddings = 256  # max no. of positions to be calculated by RoPE\\\n",
    "rope_theta = 10000  # base for the formula to calculate frequencies for RoPE, controlling the timescale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d68c14",
   "metadata": {},
   "source": [
    "#### Normalization and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a3100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm_eps = 1e-5 # to normalise the vector embeddings\n",
    "attention_bias = 0  # 0 to keep it as a Linear Layer without an extra bias vector\n",
    "attention_dropout = 0  # Dropout probability for attention weights to prevent overfitting, for simplicity, we won't use that\n",
    "use_qk_norm = True  # To apply L2 normalization on Q & K before attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe510f59",
   "metadata": {},
   "source": [
    "#### Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d3845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b2c4751a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df662603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # two independent sequences of text\n",
    "sequence_length = 10  # length of each sequence\n",
    "\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)  # creating sample input token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321974b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5043, -0.4161, -0.1364,  ..., -1.5856, -0.4089, -2.8163],\n",
      "         [ 1.0667, -0.0923,  0.3463,  ...,  0.5123,  1.9678, -1.6733],\n",
      "         [ 1.2775,  0.2651, -0.5682,  ..., -0.2129, -1.4258, -1.2878],\n",
      "         ...,\n",
      "         [ 1.4049, -0.0547, -0.4749,  ...,  2.6301, -0.4774,  0.3909],\n",
      "         [-0.5966,  0.7187, -0.3401,  ..., -0.5780,  0.9983,  0.6903],\n",
      "         [-0.4571,  0.7204,  0.3816,  ...,  1.9020, -0.6863,  0.4856]],\n",
      "\n",
      "        [[-1.8869,  2.0450, -0.3714,  ..., -0.0561,  1.2780, -0.0363],\n",
      "         [ 0.2985,  1.5429,  1.3085,  ...,  0.2492,  0.6134,  0.5383],\n",
      "         [-0.2063, -2.8666, -1.4368,  ..., -0.6156, -0.6485,  0.1808],\n",
      "         ...,\n",
      "         [-0.2064,  2.1962,  1.2381,  ...,  1.1080, -0.6104,  0.7092],\n",
      "         [-1.1046, -0.1936,  0.0943,  ..., -0.0681,  0.0745,  1.0041],\n",
      "         [ 0.8959,  0.1819,  1.3658,  ...,  0.7530, -0.9845, -0.2993]]])\n",
      "torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6f090",
   "metadata": {},
   "source": [
    "The job of `position_ids` is to tell the RoPE function the position of each token (is it the 1st, 2nd, 3rd, etc. token?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3166683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create positional ids for the tokens (very imp for RoPE)\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4ddbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.arange(0, sequence_length) : \n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "torch.arange(0, sequence_length).unsqueeze(0) : \n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "\n",
      "torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) : \n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# understanding what goes on in each step\n",
    "print(f'torch.arange(0, sequence_length) : \\n {torch.arange(0, sequence_length)}')  # creates a simple indexing sequence of numbers\n",
    "print()\n",
    "print(f'torch.arange(0, sequence_length).unsqueeze(0) : \\n {torch.arange(0, sequence_length).unsqueeze(0)}')  # Adds a new dimension of size 1 at the specified position (0-row, 1-column)\n",
    "print()\n",
    "print(f'torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) : \\n {torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)}')  # copies the sequence for each item in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65335a4a",
   "metadata": {},
   "source": [
    "#### Attention Mask\n",
    "To prevent the model from attending to the tokens after the current token, it should be able to see only the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813b8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "# create a square and assign -inf to all the upper triangle positions so the softmax functino will make it 0\n",
    "# diagonal = 1 specifies that the digonal right above the principal diagonal\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7860c7",
   "metadata": {},
   "source": [
    "* `.unsqueeze(0)` changes the dimension from `[sequence_length, sequence_length]` to `[1, sequence_length, sequence_length]`  \n",
    "\n",
    "* second `.unsqueeze(0)` changes the dimension from `[1, sequence_length, sequence_length]` to `[1, 1, sequence_length, sequence_length]`   \n",
    "\n",
    "We do this to match the dimensions with the attention_weights, which has a 4D shape `[batch_size, num_attention_heads, sequence_length, sequence_length]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e61acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attention_mask =attention_mask.unsqueeze(0).unsqueeze(0)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5108108",
   "metadata": {},
   "source": [
    "* `1` indicates that the size of the second dimension is 1. We apply the same attention mask across all attention heads. \n",
    "* `-1` indicates to not change the third and fourth dimensions, to remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb2fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7a18b",
   "metadata": {},
   "source": [
    "### Final Config Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f6e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration\n",
      "hidden_size: 128\n",
      "num_attention_heads: 16\n",
      "num_key_value_heads: 4\n",
      "head_dim: 8\n",
      "\n",
      "Sample Input Shapes\n",
      "hidden_states: torch.Size([2, 10, 128])\n",
      "position_ids: torch.Size([2, 10])\n",
      "attention_mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print('Configuration')\n",
    "print(f'hidden_size: {hidden_size}')\n",
    "print(f'num_attention_heads: {num_attention_heads}')\n",
    "print(f'num_key_value_heads: {num_key_value_heads}')\n",
    "print(f'head_dim: {head_dim}')\n",
    "print()\n",
    "print('Sample Input Shapes')\n",
    "print(f'hidden_states: {hidden_states.shape}')\n",
    "print(f'position_ids: {position_ids.shape}')\n",
    "print(f'attention_mask: {attention_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fc5f9",
   "metadata": {},
   "source": [
    "# Q, K, V Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595df0b",
   "metadata": {},
   "source": [
    "#### Define Projection Layers\n",
    "here, we define the matrices W<sup>q</sup>, W<sup>k</sup>, and W<sup>v</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df232e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias= attention_bias)\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias= attention_bias)\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias= attention_bias)\n",
    "\n",
    "# contains the learned weight matrix often refferd to as W_o on paper\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias= attention_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148a6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection layers\n",
      "q_proj: Linear(in_features=128, out_features=128, bias=False)\n",
      "k_proj: Linear(in_features=128, out_features=32, bias=False)\n",
      "v_proj: Linear(in_features=128, out_features=32, bias=False)\n",
      "o_proj: Linear(in_features=128, out_features=128, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print('Projection layers')\n",
    "print(f'q_proj: {q_proj}')\n",
    "print(f'k_proj: {k_proj}')\n",
    "print(f'v_proj: {v_proj}')\n",
    "print(f'o_proj: {o_proj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade05df",
   "metadata": {},
   "source": [
    "#### Project the input on these matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c44d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = q_proj(hidden_states)\n",
    "key_states = k_proj(hidden_states)\n",
    "value_states = v_proj(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359692ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections shape\n",
      "query_states: torch.Size([2, 10, 128])\n",
      "key_states: torch.Size([2, 10, 32])\n",
      "value_states: torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "print('Projections shape')\n",
    "print(f'query_states: {query_states.shape}')\n",
    "print(f'key_states: {key_states.shape}')\n",
    "print(f'value_states: {value_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497963f",
   "metadata": {},
   "source": [
    "* Query: We have 2 sequences, 10 tokens in each sequence, and 128 values to represent each single token.  \n",
    "* Key & Value: We have 2 sequences, 10 tokens in each sequence, and 32 values to represent each single token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bac383",
   "metadata": {},
   "source": [
    "#### Creating Individual Heads\n",
    "We have have Q, K, and V. We must divide them into individual heads for multi-head attention.  \n",
    "\n",
    "Target shape: [batch_size, num_heads, sequence_length, head_size]\n",
    "\n",
    "`view()` function is used to reshape the tensor *(works only on contiguous tensors)*  \n",
    "*we used it to split `hidden_size` dimension into two new dimensions `(num_attention_heads, head_dim)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7cf0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1,2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1,2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d1abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individual heads shapes:\n",
      "query_states: torch.Size([2, 16, 10, 8])\n",
      "key_states: torch.Size([2, 4, 10, 8])\n",
      "value_states: torch.Size([2, 4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print('The individual heads shapes:')\n",
    "print(f'query_states: {query_states.shape}')\n",
    "print(f'key_states: {key_states.shape}')\n",
    "print(f'value_states: {value_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f6e9a",
   "metadata": {},
   "source": [
    "#### Calculating the number of Query heads per Key-Value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c67458ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Key-Value groups (Q heads per K-V head): 4\n"
     ]
    }
   ],
   "source": [
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f'Number of Key-Value groups (Q heads per K-V head): {num_key_value_groups}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94d147",
   "metadata": {},
   "source": [
    "# Rotary Position Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0531c",
   "metadata": {},
   "source": [
    "#### Defining Rotation Calculation Function - `simple_rope_calculation()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "270891f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rope_calculation(dim, max_seq_len, base=10000.0, device=None):\n",
    "    \n",
    "    # Please find the breakdown of whats happening in each line below in the next cell\n",
    "\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))  # LINE 1\n",
    "    t = torch.arange(max_seq_len, device=device).type_as(inv_freq)  # LINE 2\n",
    "    freqs = torch.outer(t, inv_freq)  # LINE 3\n",
    "    emb = torch.cat((freqs, freqs), dim=1)  # LINE 4 \n",
    "    \n",
    "    # To create rotators for \"cos(theta) + i*sin(theta)\"\n",
    "    freqs_cos = emb.cos()  # real part\n",
    "    freqs_sin = emb.sin()  # imaginery part\n",
    "    freqs_cis = torch.complex(freqs_cos, freqs_sin)  # LINE 5\n",
    "\n",
    "    return freqs_cis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21457c",
   "metadata": {},
   "source": [
    "#### Breakdown of `simple_rope_calculation()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d489aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables- dim:8, device:None, base:10000.0, max_seq_len:10\n"
     ]
    }
   ],
   "source": [
    "dim = 8\n",
    "device = None\n",
    "base=10000.0\n",
    "max_seq_len = 10\n",
    "print(f'variables- dim:{dim}, device:{device}, base:{base}, max_seq_len:{max_seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a8673",
   "metadata": {},
   "source": [
    "LINE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8c9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create a vector\n",
      "tensor([0., 2., 4., 6.])\n",
      "\n",
      "To normalise, we divide by the dimension\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500])\n",
      "\n",
      "Raising base to the power of the normalized vector\n",
      "tensor([   1.,   10.,  100., 1000.])\n",
      "\n",
      "Divide 1 by all these (reciprocal)\n",
      "tensor([1.0000, 0.1000, 0.0100, 0.0010])\n"
     ]
    }
   ],
   "source": [
    "print('\\nCreate a vector')\n",
    "print(torch.arange(0, dim, 2, device=device).float())\n",
    "\n",
    "# To normalise, we divide by the dimension\n",
    "print('\\nTo normalise, we divide by the dimension')\n",
    "print(torch.arange(0, dim, 2, device=device).float() / dim)\n",
    "\n",
    "# raising base to the power\n",
    "print('\\nRaising base to the power of the normalized vector')\n",
    "print(base ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "\n",
    "# divide 1 by all these\n",
    "print('\\nDivide 1 by all these (reciprocal)')\n",
    "print(1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim)))\n",
    "\n",
    "# full line\n",
    "inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31370077",
   "metadata": {},
   "source": [
    "LINE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e9b7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t' shows the position for each token: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(max_seq_len, device=device).type_as(inv_freq)\n",
    "print(f'\\'t\\' shows the position for each token: {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d338cef",
   "metadata": {},
   "source": [
    "LINE 3  \n",
    "\n",
    "We find the outer product of `k` and `inv_freq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41576f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03],\n",
      "        [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03],\n",
      "        [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "        [4.0000e+00, 4.0000e-01, 4.0000e-02, 4.0000e-03],\n",
      "        [5.0000e+00, 5.0000e-01, 5.0000e-02, 5.0000e-03],\n",
      "        [6.0000e+00, 6.0000e-01, 6.0000e-02, 6.0000e-03],\n",
      "        [7.0000e+00, 7.0000e-01, 7.0000e-02, 7.0000e-03],\n",
      "        [8.0000e+00, 8.0000e-01, 8.0000e-02, 8.0000e-03],\n",
      "        [9.0000e+00, 9.0000e-01, 9.0000e-02, 9.0000e-03]])\n",
      "torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "freqs = torch.outer(t, inv_freq)\n",
    "print(freqs)\n",
    "print(freqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067c8f2",
   "metadata": {},
   "source": [
    "Rows represents the tokens, and the columns represents the pairs of embeddings for each token.  \n",
    "*Example:*  \n",
    "* *First token: (No rotation ) 0.0000e+00*\n",
    "* *The second token:*\n",
    "    * *first pair will rotate by 1.0000e+00*\n",
    "    * *Second pair will rotate by 1.0000e-01*\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd83f0b",
   "metadata": {},
   "source": [
    "LINE 4  \n",
    "\n",
    "We calulated only 4 frequencies for the rotations, but in reality we have `dim=8` embeddings so we need 8 angles. Hence, we achieve this by replicating the `freqs` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73355a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "emb = torch.cat((freqs, freqs), dim=1)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68899b5d",
   "metadata": {},
   "source": [
    "LINE 5  \n",
    "\n",
    "`freqs_cis` frequencies in the form of cosine + i*sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba909582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,\n",
      "          1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j,\n",
      "          0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "        [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j,\n",
      "         -0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "        [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j,\n",
      "         -0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "        [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j,\n",
      "         -0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "        [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j,\n",
      "          0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "        [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j,\n",
      "          0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "        [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j,\n",
      "          0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "        [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j,\n",
      "         -0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "        [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j,\n",
      "         -0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]])\n",
      "torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "# To create rotators for \"cos(theta) + i*sin(theta)\"\n",
    "freqs_cos = emb.cos()  # real part\n",
    "freqs_sin = emb.sin()  # imaginery part\n",
    "freqs_cis = torch.complex(freqs_cos, freqs_sin)\n",
    "print(freqs_cis)\n",
    "print(freqs_cis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9fbc35",
   "metadata": {},
   "source": [
    "#### Defining Rotations Function - `apply_rotary_emb_torch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18b90584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb_torch(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    # making sure \"freqs_cis\" is on the right device\n",
    "    freqs_cis = freqs_cis.to(xq.device)\n",
    "\n",
    "    # intead of having all the frequencies, we filter out the ones for the correspondings tokens we need.  \n",
    "    freqs_cis = freqs_cis[position_ids]  # LINE 6\n",
    "\n",
    "    freqs_cis = freqs_cis[:, None, :, :]  # LINE 7\n",
    "\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # LINE 8\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]]\n",
    "\n",
    "    # Applying the rotations\n",
    "    xq_rotated = xq_ * freqs_cis_broadcast\n",
    "    xk_rotated = xq_ * freqs_cis_broadcast\n",
    "\n",
    "    # Convert back to real representatinos\n",
    "    # This flattens the last 2 dimensions back into one.\n",
    "    # input: [batch, num_heads, seq_length, head_dim / 2 (4), 2] (complex)\n",
    "    # output: [batch, num_heads, seq_length, 8]\n",
    "    xq_out = torch.view_as_real(xq_rotated).flatten(3) \n",
    "    xk_out = torch.view_as_real(xk_rotated).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924579e9",
   "metadata": {},
   "source": [
    "#### Breakdown of `apply_rotary_emb_torch()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cf114",
   "metadata": {},
   "source": [
    "LINE 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e60cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs_cis shape: torch.Size([10, 8])\n",
      "position_ids shape: torch.Size([2, 10])\n",
      "freqs_cis[position_ids] shape: torch.Size([2, 10, 8])\n",
      "final shape: \n",
      "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,\n",
      "          1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j,\n",
      "          0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "        [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j,\n",
      "         -0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "        [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j,\n",
      "         -0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "        [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j,\n",
      "         -0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "        [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j,\n",
      "          0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "        [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j,\n",
      "          0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "        [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j,\n",
      "          0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "        [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j,\n",
      "         -0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "        [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j,\n",
      "         -0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]])\n"
     ]
    }
   ],
   "source": [
    "print(f'freqs_cis shape: {freqs_cis.shape}')\n",
    "print(f'position_ids shape: {position_ids.shape}')\n",
    "print(f'freqs_cis[position_ids] shape: {freqs_cis[position_ids].shape}')\n",
    "print(f'final shape: \\n{freqs_cis[position_ids][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c19110",
   "metadata": {},
   "source": [
    "LINE 7\n",
    "  \n",
    "The Query and Key tensors have a shape of `[batch, num_heads, seq_len, head_dim]`. So, we add a dimension of `1` to `freqs_cis` to align with Q & K so we can broadcast them with this angles tensor.  \n",
    "\n",
    "Can use `unsqueeze(1)` too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d02ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8])\n",
      "torch.Size([2, 1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "freqs_cis = freqs_cis[position_ids]\n",
    "print(freqs_cis.shape)\n",
    "freqs_cis = freqs_cis[:, None, :, :]\n",
    "print(freqs_cis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d6cb3",
   "metadata": {},
   "source": [
    "LINE 8  \n",
    "\n",
    "Now, we reshapre the Query and Key tensors so that the pairs of numbers are treated as complex numbers **(a+ib)**.  \n",
    "*Same process for \"xk\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73c3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.8011, -0.5517,  1.2441,  ..., -0.8717, -0.2942,  0.3337],\n",
      "          [ 0.1762, -0.1274, -0.8008,  ..., -1.1549, -0.4158, -1.7951],\n",
      "          [-2.3783, -0.9613, -2.1015,  ...,  1.3007, -1.0858,  0.3678],\n",
      "          ...,\n",
      "          [-0.5342,  0.1280, -0.4700,  ...,  0.9925,  0.6600,  1.5047],\n",
      "          [ 0.6374,  1.0656,  1.0083,  ...,  1.0599,  1.4468, -0.8652],\n",
      "          [ 0.0680,  0.1107, -1.6959,  ...,  0.9235,  1.0702,  0.6899]],\n",
      "\n",
      "         [[ 1.5018,  0.9565, -1.0583,  ...,  0.8433, -0.3213,  1.2631],\n",
      "          [ 0.7975, -0.1302, -0.2862,  ...,  1.8829,  0.0470,  1.5126],\n",
      "          [-0.1961,  0.0146,  0.3894,  ..., -0.9640,  0.0291, -0.8071],\n",
      "          ...,\n",
      "          [-0.4551, -0.5375,  1.8866,  ...,  0.1935, -0.1347, -0.1596],\n",
      "          [ 1.0800, -2.1215, -1.3935,  ..., -0.1580, -0.9938, -1.1936],\n",
      "          [ 0.4865,  0.2801, -0.1601,  ...,  0.4223,  0.5414,  0.1586]],\n",
      "\n",
      "         [[ 0.5211,  0.2147,  1.4402,  ...,  1.8647, -1.8323,  1.3069],\n",
      "          [ 0.3001,  0.2398, -0.9430,  ..., -0.8664,  1.0388,  1.0751],\n",
      "          [-1.3928,  0.2338,  0.4019,  ...,  0.1353,  0.0426,  0.9569],\n",
      "          ...,\n",
      "          [ 1.5050, -0.1753,  0.4697,  ...,  1.2028,  1.0958,  0.6408],\n",
      "          [ 0.6259,  0.0310,  2.0470,  ...,  1.0573, -0.0413,  0.4832],\n",
      "          [-1.0886, -1.5154,  1.2020,  ...,  0.1804,  0.0945, -0.9067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2940,  0.0462, -0.6973,  ...,  0.9776,  0.3430, -1.6760],\n",
      "          [-0.0867, -0.9195,  0.6667,  ..., -3.4245,  0.2251, -0.2830],\n",
      "          [ 0.4394, -0.0074,  1.0045,  ..., -0.3994, -1.0095,  2.0196],\n",
      "          ...,\n",
      "          [ 1.2348, -2.0090, -0.2044,  ...,  1.6067, -0.4039, -0.1558],\n",
      "          [ 1.5133,  0.3667,  0.0079,  ...,  0.6463,  0.5581,  0.0629],\n",
      "          [ 0.4626,  0.2977,  1.3681,  ..., -0.4673, -1.9059,  0.5545]],\n",
      "\n",
      "         [[ 1.0854,  0.7462, -0.0888,  ..., -0.1235, -0.6239,  0.4802],\n",
      "          [ 1.1291, -0.0379,  0.4666,  ...,  0.7060, -0.2923, -0.5499],\n",
      "          [-0.8288,  1.3542, -0.3524,  ..., -0.5295, -1.3424, -1.1722],\n",
      "          ...,\n",
      "          [ 2.0386, -1.2172, -0.0626,  ...,  0.2755, -0.0870,  0.5118],\n",
      "          [ 1.2932, -1.3931, -0.6286,  ...,  0.8451, -1.6330,  0.2341],\n",
      "          [ 0.7883, -1.7499,  0.1999,  ..., -1.0476,  0.3095,  1.7801]],\n",
      "\n",
      "         [[ 0.0162,  0.0055,  0.6429,  ...,  0.3824, -1.3283,  1.4842],\n",
      "          [-1.6005,  2.1088, -0.5929,  ...,  0.5081,  0.7192, -0.2037],\n",
      "          [ 0.5617, -1.8397, -1.4682,  ..., -1.4761, -1.0822, -0.2580],\n",
      "          ...,\n",
      "          [ 0.9833,  0.2792, -1.8523,  ..., -0.7130,  0.2385,  0.1781],\n",
      "          [ 0.8940,  1.3886,  1.3564,  ..., -0.3522, -2.4223, -1.1826],\n",
      "          [-1.0077,  1.3152,  1.0974,  ...,  0.7682, -0.7211, -1.5168]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5922, -0.6350,  0.4461,  ...,  0.6067, -1.7861, -1.8030],\n",
      "          [-1.3734,  2.0454, -0.1976,  ...,  1.1355, -0.0404, -1.3656],\n",
      "          [ 0.9154,  1.1340,  0.5651,  ..., -1.1531,  0.5761,  0.9172],\n",
      "          ...,\n",
      "          [-1.4368,  1.2994, -1.3753,  ...,  2.0309, -0.6752,  1.4835],\n",
      "          [ 0.4674, -1.2331,  1.6251,  ...,  0.6961,  0.4998, -0.0837],\n",
      "          [-0.4126,  0.4584,  1.1674,  ...,  0.0532,  2.7045,  1.1794]],\n",
      "\n",
      "         [[-0.1506, -0.6920, -1.1618,  ...,  0.8197,  0.3263, -0.4676],\n",
      "          [ 0.2625,  2.2612,  1.1514,  ..., -0.0615, -2.3085, -0.8576],\n",
      "          [-1.0555, -0.9648, -0.0151,  ..., -0.8415,  0.6684, -0.5485],\n",
      "          ...,\n",
      "          [-2.0134, -1.2888, -1.3781,  ...,  0.1670,  0.1205,  1.1772],\n",
      "          [ 0.2083,  0.0145, -2.4004,  ..., -0.8994,  0.1144,  0.1177],\n",
      "          [-0.0492, -2.4687, -0.9145,  ..., -0.4519,  0.2912,  0.3972]],\n",
      "\n",
      "         [[ 0.1735,  1.4142, -1.1889,  ..., -0.5126,  1.2230, -0.1668],\n",
      "          [ 0.1898,  0.3021,  1.0851,  ...,  2.1549,  0.5525,  0.0098],\n",
      "          [ 0.9288,  1.4002,  1.7312,  ...,  1.9116, -0.3867,  1.8782],\n",
      "          ...,\n",
      "          [-0.0077, -0.6147, -0.2113,  ..., -0.2046, -0.6037, -1.1615],\n",
      "          [-1.0848,  1.3818, -0.5008,  ...,  2.2552, -1.7400,  1.3897],\n",
      "          [ 0.1582,  2.8404, -0.7392,  ...,  0.9959, -0.3989,  0.8601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4464,  0.3756,  1.5236,  ..., -0.2831, -0.1253,  0.5725],\n",
      "          [ 0.7072, -0.5884, -1.6144,  ..., -1.7085,  0.2312, -0.8767],\n",
      "          [ 0.8969,  0.6809, -0.0917,  ...,  1.1542, -0.2700,  0.7507],\n",
      "          ...,\n",
      "          [ 1.3691, -0.6322,  0.5203,  ...,  1.2622, -0.6691,  1.3126],\n",
      "          [-0.0443, -1.4293, -0.3599,  ...,  1.0354,  0.7702,  0.2818],\n",
      "          [-0.1565, -0.6232,  0.6213,  ...,  0.0395,  0.7935, -0.5916]],\n",
      "\n",
      "         [[-0.3095,  1.4352, -0.0814,  ...,  0.6601,  0.2587, -1.9377],\n",
      "          [-0.2076, -1.2934, -0.4961,  ..., -1.5821,  0.8282,  0.1264],\n",
      "          [ 0.6118,  0.2429, -0.4984,  ..., -0.2268, -0.9570, -1.1678],\n",
      "          ...,\n",
      "          [ 0.3962, -0.9689, -0.3834,  ..., -0.6925,  0.6691, -0.3638],\n",
      "          [-2.5463,  0.2418,  2.1431,  ..., -0.3108, -1.2682, -1.8996],\n",
      "          [-1.2378,  0.4145, -0.3340,  ..., -1.3132,  0.6186,  0.7881]],\n",
      "\n",
      "         [[ 0.1627, -0.9479,  0.2914,  ...,  0.4107, -1.6072, -0.5889],\n",
      "          [-0.1229,  1.2364,  0.5439,  ...,  0.5870,  0.0140, -0.2195],\n",
      "          [-1.8897,  1.0495,  0.4650,  ...,  1.5156, -0.6029,  0.8890],\n",
      "          ...,\n",
      "          [-0.0067,  0.6587, -1.5780,  ...,  1.1813, -0.2504, -0.8904],\n",
      "          [ 1.0538,  0.9510, -0.2236,  ...,  0.0054, -2.1030, -0.6985],\n",
      "          [ 0.7926,  0.9932,  0.5485,  ...,  0.2563, -0.4201, -0.5072]]]])\n",
      "torch.Size([2, 16, 10, 8])\n",
      "\n",
      "tensor([[[[ 0.8011, -0.5517,  1.2441,  ..., -0.8717, -0.2942,  0.3337],\n",
      "          [ 0.1762, -0.1274, -0.8008,  ..., -1.1549, -0.4158, -1.7951],\n",
      "          [-2.3783, -0.9613, -2.1015,  ...,  1.3007, -1.0858,  0.3678],\n",
      "          ...,\n",
      "          [-0.5342,  0.1280, -0.4700,  ...,  0.9925,  0.6600,  1.5047],\n",
      "          [ 0.6374,  1.0656,  1.0083,  ...,  1.0599,  1.4468, -0.8652],\n",
      "          [ 0.0680,  0.1107, -1.6959,  ...,  0.9235,  1.0702,  0.6899]],\n",
      "\n",
      "         [[ 1.5018,  0.9565, -1.0583,  ...,  0.8433, -0.3213,  1.2631],\n",
      "          [ 0.7975, -0.1302, -0.2862,  ...,  1.8829,  0.0470,  1.5126],\n",
      "          [-0.1961,  0.0146,  0.3894,  ..., -0.9640,  0.0291, -0.8071],\n",
      "          ...,\n",
      "          [-0.4551, -0.5375,  1.8866,  ...,  0.1935, -0.1347, -0.1596],\n",
      "          [ 1.0800, -2.1215, -1.3935,  ..., -0.1580, -0.9938, -1.1936],\n",
      "          [ 0.4865,  0.2801, -0.1601,  ...,  0.4223,  0.5414,  0.1586]],\n",
      "\n",
      "         [[ 0.5211,  0.2147,  1.4402,  ...,  1.8647, -1.8323,  1.3069],\n",
      "          [ 0.3001,  0.2398, -0.9430,  ..., -0.8664,  1.0388,  1.0751],\n",
      "          [-1.3928,  0.2338,  0.4019,  ...,  0.1353,  0.0426,  0.9569],\n",
      "          ...,\n",
      "          [ 1.5050, -0.1753,  0.4697,  ...,  1.2028,  1.0958,  0.6408],\n",
      "          [ 0.6259,  0.0310,  2.0470,  ...,  1.0573, -0.0413,  0.4832],\n",
      "          [-1.0886, -1.5154,  1.2020,  ...,  0.1804,  0.0945, -0.9067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2940,  0.0462, -0.6973,  ...,  0.9776,  0.3430, -1.6760],\n",
      "          [-0.0867, -0.9195,  0.6667,  ..., -3.4245,  0.2251, -0.2830],\n",
      "          [ 0.4394, -0.0074,  1.0045,  ..., -0.3994, -1.0095,  2.0196],\n",
      "          ...,\n",
      "          [ 1.2348, -2.0090, -0.2044,  ...,  1.6067, -0.4039, -0.1558],\n",
      "          [ 1.5133,  0.3667,  0.0079,  ...,  0.6463,  0.5581,  0.0629],\n",
      "          [ 0.4626,  0.2977,  1.3681,  ..., -0.4673, -1.9059,  0.5545]],\n",
      "\n",
      "         [[ 1.0854,  0.7462, -0.0888,  ..., -0.1235, -0.6239,  0.4802],\n",
      "          [ 1.1291, -0.0379,  0.4666,  ...,  0.7060, -0.2923, -0.5499],\n",
      "          [-0.8288,  1.3542, -0.3524,  ..., -0.5295, -1.3424, -1.1722],\n",
      "          ...,\n",
      "          [ 2.0386, -1.2172, -0.0626,  ...,  0.2755, -0.0870,  0.5118],\n",
      "          [ 1.2932, -1.3931, -0.6286,  ...,  0.8451, -1.6330,  0.2341],\n",
      "          [ 0.7883, -1.7499,  0.1999,  ..., -1.0476,  0.3095,  1.7801]],\n",
      "\n",
      "         [[ 0.0162,  0.0055,  0.6429,  ...,  0.3824, -1.3283,  1.4842],\n",
      "          [-1.6005,  2.1088, -0.5929,  ...,  0.5081,  0.7192, -0.2037],\n",
      "          [ 0.5617, -1.8397, -1.4682,  ..., -1.4761, -1.0822, -0.2580],\n",
      "          ...,\n",
      "          [ 0.9833,  0.2792, -1.8523,  ..., -0.7130,  0.2385,  0.1781],\n",
      "          [ 0.8940,  1.3886,  1.3564,  ..., -0.3522, -2.4223, -1.1826],\n",
      "          [-1.0077,  1.3152,  1.0974,  ...,  0.7682, -0.7211, -1.5168]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5922, -0.6350,  0.4461,  ...,  0.6067, -1.7861, -1.8030],\n",
      "          [-1.3734,  2.0454, -0.1976,  ...,  1.1355, -0.0404, -1.3656],\n",
      "          [ 0.9154,  1.1340,  0.5651,  ..., -1.1531,  0.5761,  0.9172],\n",
      "          ...,\n",
      "          [-1.4368,  1.2994, -1.3753,  ...,  2.0309, -0.6752,  1.4835],\n",
      "          [ 0.4674, -1.2331,  1.6251,  ...,  0.6961,  0.4998, -0.0837],\n",
      "          [-0.4126,  0.4584,  1.1674,  ...,  0.0532,  2.7045,  1.1794]],\n",
      "\n",
      "         [[-0.1506, -0.6920, -1.1618,  ...,  0.8197,  0.3263, -0.4676],\n",
      "          [ 0.2625,  2.2612,  1.1514,  ..., -0.0615, -2.3085, -0.8576],\n",
      "          [-1.0555, -0.9648, -0.0151,  ..., -0.8415,  0.6684, -0.5485],\n",
      "          ...,\n",
      "          [-2.0134, -1.2888, -1.3781,  ...,  0.1670,  0.1205,  1.1772],\n",
      "          [ 0.2083,  0.0145, -2.4004,  ..., -0.8994,  0.1144,  0.1177],\n",
      "          [-0.0492, -2.4687, -0.9145,  ..., -0.4519,  0.2912,  0.3972]],\n",
      "\n",
      "         [[ 0.1735,  1.4142, -1.1889,  ..., -0.5126,  1.2230, -0.1668],\n",
      "          [ 0.1898,  0.3021,  1.0851,  ...,  2.1549,  0.5525,  0.0098],\n",
      "          [ 0.9288,  1.4002,  1.7312,  ...,  1.9116, -0.3867,  1.8782],\n",
      "          ...,\n",
      "          [-0.0077, -0.6147, -0.2113,  ..., -0.2046, -0.6037, -1.1615],\n",
      "          [-1.0848,  1.3818, -0.5008,  ...,  2.2552, -1.7400,  1.3897],\n",
      "          [ 0.1582,  2.8404, -0.7392,  ...,  0.9959, -0.3989,  0.8601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4464,  0.3756,  1.5236,  ..., -0.2831, -0.1253,  0.5725],\n",
      "          [ 0.7072, -0.5884, -1.6144,  ..., -1.7085,  0.2312, -0.8767],\n",
      "          [ 0.8969,  0.6809, -0.0917,  ...,  1.1542, -0.2700,  0.7507],\n",
      "          ...,\n",
      "          [ 1.3691, -0.6322,  0.5203,  ...,  1.2622, -0.6691,  1.3126],\n",
      "          [-0.0443, -1.4293, -0.3599,  ...,  1.0354,  0.7702,  0.2818],\n",
      "          [-0.1565, -0.6232,  0.6213,  ...,  0.0395,  0.7935, -0.5916]],\n",
      "\n",
      "         [[-0.3095,  1.4352, -0.0814,  ...,  0.6601,  0.2587, -1.9377],\n",
      "          [-0.2076, -1.2934, -0.4961,  ..., -1.5821,  0.8282,  0.1264],\n",
      "          [ 0.6118,  0.2429, -0.4984,  ..., -0.2268, -0.9570, -1.1678],\n",
      "          ...,\n",
      "          [ 0.3962, -0.9689, -0.3834,  ..., -0.6925,  0.6691, -0.3638],\n",
      "          [-2.5463,  0.2418,  2.1431,  ..., -0.3108, -1.2682, -1.8996],\n",
      "          [-1.2378,  0.4145, -0.3340,  ..., -1.3132,  0.6186,  0.7881]],\n",
      "\n",
      "         [[ 0.1627, -0.9479,  0.2914,  ...,  0.4107, -1.6072, -0.5889],\n",
      "          [-0.1229,  1.2364,  0.5439,  ...,  0.5870,  0.0140, -0.2195],\n",
      "          [-1.8897,  1.0495,  0.4650,  ...,  1.5156, -0.6029,  0.8890],\n",
      "          ...,\n",
      "          [-0.0067,  0.6587, -1.5780,  ...,  1.1813, -0.2504, -0.8904],\n",
      "          [ 1.0538,  0.9510, -0.2236,  ...,  0.0054, -2.1030, -0.6985],\n",
      "          [ 0.7926,  0.9932,  0.5485,  ...,  0.2563, -0.4201, -0.5072]]]])\n",
      "torch.Size([2, 16, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# random xq for testing\n",
    "xq = torch.randn(batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "xk = torch.randn(batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "print(xq)\n",
    "print(f'{xq.shape}\\n')\n",
    "\n",
    "\n",
    "xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # keeps dimensions execpt the last one same, then splits the last one into two\n",
    "print(xq)\n",
    "print(xq.shape)\n",
    "\n",
    "xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))  # keeps dimensions execpt the last one same, then splits the last one into two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9c6fc",
   "metadata": {},
   "source": [
    "*8 has changed to 4 because we divided the deimension into pairs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30322f",
   "metadata": {},
   "source": [
    "LINE 9  \n",
    "\n",
    "Previsously, we calculated the `freqs_cis` tensor by using `head_dim` sines and cosines. However, xq's last dimension is now 4 (*Because each pair of numbers become a single complex number*).  \n",
    "To multiply them, we need freq_cis in the same dimesnions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f302e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially: torch.Size([2, 1, 10, 8])\n",
      "tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "          [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "          [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "          [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "          [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "          [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "          [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "          [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "          [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "          [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "          [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "          [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "          [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "          [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "          [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "          [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "          [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "          [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "          [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]]]])\n",
      "torch.Size([2, 1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "print(f'initially: {freqs_cis.shape}')\n",
    "\n",
    "freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]]\n",
    "print(freqs_cis_broadcast)\n",
    "print(freqs_cis_broadcast.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60bb02",
   "metadata": {},
   "source": [
    "Now, we have the rotations for 4 pairs in each token.  \n",
    "\n",
    "**NOTE:** *The slicing step is necessary because we duplicated the frequencies in the `simple_rope_calculation()` function. We do this to understand the existing implementation of transformers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9c260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "071d0c59",
   "metadata": {},
   "source": [
    "#### Calculating RoPE Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aad3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated freqs_cis shape: torch.Size([256, 8])\n"
     ]
    }
   ],
   "source": [
    "# pre-computed frequencies depending solely on the position, and not the token.\n",
    "freqs_cis = simple_rope_calculation(head_dim, max_positional_embeddings, base=rope_theta, device=hidden_states.device)\n",
    "print(f'Calculated freqs_cis shape: {freqs_cis.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107c8b2",
   "metadata": {},
   "source": [
    "#### Applying RoPE Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2392bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after RoPE:\n",
      "  query_states_rope: torch.Size([2, 16, 10, 8])\n",
      "  key_states_rope: torch.Size([2, 16, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, freqs_cis)\n",
    "\n",
    "print('Shapes after RoPE:')\n",
    "print(f'  query_states_rope: {query_states_rope.shape}')\n",
    "print(f'  key_states_rope: {key_states_rope.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26e638",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d68a9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleL2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps # to avoid div by 0 during normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd54679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
