{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948ca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Coding\\Python\\llama-from-scratch\\venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75108c81",
   "metadata": {},
   "source": [
    "# Config settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5378bf1",
   "metadata": {},
   "source": [
    "#### Architecural Dimensions\n",
    "$\\underline{\\text{Grouped-Query Attention (GQA)}}$  \n",
    "This is a technique in which we use fewer number of Key/Value heads than the Query heads.  This method requires significantly less memory, and can generate text much faster with a very small impact on the overall accuracy.  \n",
    "> __note:__ The number of Query heads must be perfectly divisble by the number of Key/Value heads.  \n",
    "\n",
    "*In this case, we use one Key and Value heads per 4 Query heads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fa0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 128   # synonymous with embeddings dimension\n",
    "num_attention_heads = 16    # The no. of attention query heads\n",
    "num_key_value_heads = 4     # The no. of Key & Value heads [Grouped-Query Attention (GQA)]\n",
    "head_dim = hidden_size // num_attention_heads   # Dimension of each atention head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31279561",
   "metadata": {},
   "source": [
    "#### Positional Embedding Parameters\n",
    "\n",
    "Instead of traditional positional embeddings, this model uses RoPE to encode the order of tokens. RoPE modifies the Query and Key vectors using rotations, which elegantly injects relative positional information directly into the self-attention calculation.  \n",
    "\n",
    "\n",
    "$\\underline{\\text{Rotary Positional Encodings (RoPE)}}$  \n",
    "* **Relative Position:** The attention score between two tokens becomes sensitive to their relative distance, not their absolute positions.\n",
    "* **No Trainable Parameters:** Positional information is added via a deterministic function, requiring no extra parameters to be learned.\n",
    "* **Long Sequence Extrapolation:** RoPE has been shown to be effective at handling sequences longer than the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3aea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positional_embeddings = 256  # max no. of positions to be calculated by RoPE\\\n",
    "rope_theta = 10000  # base for the formula to calculate frequencies for RoPE, controlling the timescale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d68c14",
   "metadata": {},
   "source": [
    "#### Normalization and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a3100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm_eps = 1e-5 # to normalise the vector embeddings\n",
    "attention_bias = 0  # 0 to keep it as a Linear Layer without an extra bias vector\n",
    "attention_dropout = 0  # Dropout probability for attention weights to prevent overfitting, for simplicity, we won't use that\n",
    "use_qk_norm = True  # To apply L2 normalization on Q & K before attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe510f59",
   "metadata": {},
   "source": [
    "#### Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d3845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ef3f441a70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df662603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # two independent sequences of text\n",
    "sequence_length = 10  # length of each sequence\n",
    "\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)  # creating sample input token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321974b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5043, -0.4161, -0.1364,  ..., -1.5856, -0.4089, -2.8163],\n",
      "         [ 1.0667, -0.0923,  0.3463,  ...,  0.5123,  1.9678, -1.6733],\n",
      "         [ 1.2775,  0.2651, -0.5682,  ..., -0.2129, -1.4258, -1.2878],\n",
      "         ...,\n",
      "         [ 1.4049, -0.0547, -0.4749,  ...,  2.6301, -0.4774,  0.3909],\n",
      "         [-0.5966,  0.7187, -0.3401,  ..., -0.5780,  0.9983,  0.6903],\n",
      "         [-0.4571,  0.7204,  0.3816,  ...,  1.9020, -0.6863,  0.4856]],\n",
      "\n",
      "        [[-1.8869,  2.0450, -0.3714,  ..., -0.0561,  1.2780, -0.0363],\n",
      "         [ 0.2985,  1.5429,  1.3085,  ...,  0.2492,  0.6134,  0.5383],\n",
      "         [-0.2063, -2.8666, -1.4368,  ..., -0.6156, -0.6485,  0.1808],\n",
      "         ...,\n",
      "         [-0.2064,  2.1962,  1.2381,  ...,  1.1080, -0.6104,  0.7092],\n",
      "         [-1.1046, -0.1936,  0.0943,  ..., -0.0681,  0.0745,  1.0041],\n",
      "         [ 0.8959,  0.1819,  1.3658,  ...,  0.7530, -0.9845, -0.2993]]])\n",
      "torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6f090",
   "metadata": {},
   "source": [
    "The job of `position_ids` is to tell the RoPE function the position of each token (is it the 1st, 2nd, 3rd, etc. token?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3166683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create positional ids for the tokens (very imp for RoPE)\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4ddbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.arange(0, sequence_length) : \n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "torch.arange(0, sequence_length).unsqueeze(0) : \n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "\n",
      "torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) : \n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# understanding what goes on in each step\n",
    "print(f'torch.arange(0, sequence_length) : \\n {torch.arange(0, sequence_length)}')  # creates a simple indexing sequence of numbers\n",
    "print()\n",
    "print(f'torch.arange(0, sequence_length).unsqueeze(0) : \\n {torch.arange(0, sequence_length).unsqueeze(0)}')  # Adds a new dimension of size 1 at the specified position (0-row, 1-column)\n",
    "print()\n",
    "print(f'torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) : \\n {torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)}')  # copies the sequence for each item in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65335a4a",
   "metadata": {},
   "source": [
    "#### Attention Mask\n",
    "To prevent the model from attending to the tokens after the current token, it should be able to see only the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813b8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "# create a square and assign -inf to all the upper triangle positions so the softmax functino will make it 0\n",
    "# diagonal = 1 specifies that the digonal right above the principal diagonal\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7860c7",
   "metadata": {},
   "source": [
    "* `.unsqueeze(0)` changes the dimension from `[sequence_length, sequence_length]` to `[1, sequence_length, sequence_length]`  \n",
    "\n",
    "* second `.unsqueeze(0)` changes the dimension from `[1, sequence_length, sequence_length]` to `[1, 1, sequence_length, sequence_length]`   \n",
    "\n",
    "We do this to match the dimensions with the attention_weights, which has a 4D shape `[batch_size, num_attention_heads, sequence_length, sequence_length]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e61acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attention_mask =attention_mask.unsqueeze(0).unsqueeze(0)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5108108",
   "metadata": {},
   "source": [
    "* `1` indicates that the size of the second dimension is 1. We apply the same attention mask across all attention heads. \n",
    "* `-1` indicates to not change the third and fourth dimensions, to remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb2fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7a18b",
   "metadata": {},
   "source": [
    "### Final Config Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f6e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration\n",
      "hidden_size: 128\n",
      "num_attention_heads: 16\n",
      "num_key_value_heads: 4\n",
      "head_dim: 8\n",
      "\n",
      "Sample Input Shapes\n",
      "hidden_states: torch.Size([2, 10, 128])\n",
      "position_ids: torch.Size([2, 10])\n",
      "attention_mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print('Configuration')\n",
    "print(f'hidden_size: {hidden_size}')\n",
    "print(f'num_attention_heads: {num_attention_heads}')\n",
    "print(f'num_key_value_heads: {num_key_value_heads}')\n",
    "print(f'head_dim: {head_dim}')\n",
    "print()\n",
    "print('Sample Input Shapes')\n",
    "print(f'hidden_states: {hidden_states.shape}')\n",
    "print(f'position_ids: {position_ids.shape}')\n",
    "print(f'attention_mask: {attention_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fc5f9",
   "metadata": {},
   "source": [
    "# Q, K, V Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595df0b",
   "metadata": {},
   "source": [
    "#### Define Projection Layers\n",
    "here, we define the matrices W<sup>q</sup>, W<sup>k</sup>, and W<sup>v</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df232e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias= attention_bias)\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias= attention_bias)\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias= attention_bias)\n",
    "\n",
    "# contains the learned weight matrix often refferd to as W_o on paper\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias= attention_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148a6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection layers\n",
      "q_proj: Linear(in_features=128, out_features=128, bias=False)\n",
      "k_proj: Linear(in_features=128, out_features=32, bias=False)\n",
      "v_proj: Linear(in_features=128, out_features=32, bias=False)\n",
      "o_proj: Linear(in_features=128, out_features=128, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print('Projection layers')\n",
    "print(f'q_proj: {q_proj}')\n",
    "print(f'k_proj: {k_proj}')\n",
    "print(f'v_proj: {v_proj}')\n",
    "print(f'o_proj: {o_proj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade05df",
   "metadata": {},
   "source": [
    "#### Project the input on these matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c44d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = q_proj(hidden_states)\n",
    "key_states = k_proj(hidden_states)\n",
    "value_states = v_proj(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359692ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections shape\n",
      "query_states: torch.Size([2, 10, 128])\n",
      "key_states: torch.Size([2, 10, 32])\n",
      "value_states: torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "print('Projections shape')\n",
    "print(f'query_states: {query_states.shape}')\n",
    "print(f'key_states: {key_states.shape}')\n",
    "print(f'value_states: {value_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497963f",
   "metadata": {},
   "source": [
    "* Query: We have 2 sequences, 10 tokens in each sequence, and 128 values to represent each single token.  \n",
    "* Key & Value: We have 2 sequences, 10 tokens in each sequence, and 32 values to represent each single token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bac383",
   "metadata": {},
   "source": [
    "#### Creating Individual Heads\n",
    "We have have Q, K, and V. We must divide them into individual heads for multi-head attention.  \n",
    "\n",
    "Target shape: [batch_size, num_heads, sequence_length, head_size]\n",
    "\n",
    "`view()` function is used to reshape the tensor *(works only on contiguous tensors)*  \n",
    "*we used it to split `hidden_size` dimension into two new dimensions `(num_attention_heads, head_dim)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7cf0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1,2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1,2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d1abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individual heads shapes:\n",
      "query_states: torch.Size([2, 16, 10, 8])\n",
      "key_states: torch.Size([2, 4, 10, 8])\n",
      "value_states: torch.Size([2, 4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print('The individual heads shapes:')\n",
    "print(f'query_states: {query_states.shape}')\n",
    "print(f'key_states: {key_states.shape}')\n",
    "print(f'value_states: {value_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f6e9a",
   "metadata": {},
   "source": [
    "#### Calculating the number of Query heads per Key-Value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c67458ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Key-Value groups (Q heads per K-V head): 4\n"
     ]
    }
   ],
   "source": [
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f'Number of Key-Value groups (Q heads per K-V head): {num_key_value_groups}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94d147",
   "metadata": {},
   "source": [
    "# Rotary Position Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0531c",
   "metadata": {},
   "source": [
    "#### Defining Rotation Calculation Function - `simple_rope_calculation()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "270891f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rope_calculation(dim, max_seq_len, base=10000.0, device=None):\n",
    "    \n",
    "    # Please find the breakdown of whats happening in each line below in the next cell\n",
    "\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))  # LINE 1\n",
    "    t = torch.arange(max_seq_len, device=device).type_as(inv_freq)  # LINE 2\n",
    "    freqs = torch.outer(t, inv_freq)  # LINE 3\n",
    "    emb = torch.cat((freqs, freqs), dim=1)  # LINE 4 \n",
    "    \n",
    "    # To create rotators for \"cos(theta) + i*sin(theta)\"\n",
    "    freqs_cos = emb.cos()  # real part\n",
    "    freqs_sin = emb.sin()  # imaginery part\n",
    "    freqs_cis = torch.complex(freqs_cos, freqs_sin)  # LINE 5\n",
    "\n",
    "    return freqs_cis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21457c",
   "metadata": {},
   "source": [
    "#### Breakdown of `simple_rope_calculation()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d489aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables- dim:8, device:None, base:10000.0, max_seq_len:10\n"
     ]
    }
   ],
   "source": [
    "dim = 8\n",
    "device = None\n",
    "base=10000.0\n",
    "max_seq_len = 10\n",
    "print(f'variables- dim:{dim}, device:{device}, base:{base}, max_seq_len:{max_seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a8673",
   "metadata": {},
   "source": [
    "LINE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8c9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create a vector\n",
      "tensor([0., 2., 4., 6.])\n",
      "\n",
      "To normalise, we divide by the dimension\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500])\n",
      "\n",
      "Raising base to the power of the normalized vector\n",
      "tensor([   1.,   10.,  100., 1000.])\n",
      "\n",
      "Divide 1 by all these (reciprocal)\n",
      "tensor([1.0000, 0.1000, 0.0100, 0.0010])\n"
     ]
    }
   ],
   "source": [
    "print('\\nCreate a vector')\n",
    "print(torch.arange(0, dim, 2, device=device).float())\n",
    "\n",
    "# To normalise, we divide by the dimension\n",
    "print('\\nTo normalise, we divide by the dimension')\n",
    "print(torch.arange(0, dim, 2, device=device).float() / dim)\n",
    "\n",
    "# raising base to the power\n",
    "print('\\nRaising base to the power of the normalized vector')\n",
    "print(base ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "\n",
    "# divide 1 by all these\n",
    "print('\\nDivide 1 by all these (reciprocal)')\n",
    "print(1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim)))\n",
    "\n",
    "# full line\n",
    "inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31370077",
   "metadata": {},
   "source": [
    "LINE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e9b7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t' shows the position for each token: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(max_seq_len, device=device).type_as(inv_freq)\n",
    "print(f'\\'t\\' shows the position for each token: {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d338cef",
   "metadata": {},
   "source": [
    "LINE 3  \n",
    "\n",
    "We find the outer product of `k` and `inv_freq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41576f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03],\n",
      "        [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03],\n",
      "        [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "        [4.0000e+00, 4.0000e-01, 4.0000e-02, 4.0000e-03],\n",
      "        [5.0000e+00, 5.0000e-01, 5.0000e-02, 5.0000e-03],\n",
      "        [6.0000e+00, 6.0000e-01, 6.0000e-02, 6.0000e-03],\n",
      "        [7.0000e+00, 7.0000e-01, 7.0000e-02, 7.0000e-03],\n",
      "        [8.0000e+00, 8.0000e-01, 8.0000e-02, 8.0000e-03],\n",
      "        [9.0000e+00, 9.0000e-01, 9.0000e-02, 9.0000e-03]])\n",
      "torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "freqs = torch.outer(t, inv_freq)\n",
    "print(freqs)\n",
    "print(freqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067c8f2",
   "metadata": {},
   "source": [
    "Rows represents the tokens, and the columns represents the pairs of embeddings for each token.  \n",
    "*Example:*  \n",
    "* *First token: (No rotation ) 0.0000e+00*\n",
    "* *The second token:*\n",
    "    * *first pair will rotate by 1.0000e+00*\n",
    "    * *Second pair will rotate by 1.0000e-01*\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd83f0b",
   "metadata": {},
   "source": [
    "LINE 4  \n",
    "\n",
    "We calulated only 4 frequencies for the rotations, but in reality we have `dim=8` embeddings so we need 8 angles. Hence, we achieve this by replicating the `freqs` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73355a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "emb = torch.cat((freqs, freqs), dim=1)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68899b5d",
   "metadata": {},
   "source": [
    "LINE 5  \n",
    "\n",
    "`freqs_cis` frequencies in the form of cosine + i*sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba909582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,\n",
      "          1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j,\n",
      "          0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "        [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j,\n",
      "         -0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "        [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j,\n",
      "         -0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "        [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j,\n",
      "         -0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "        [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j,\n",
      "          0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "        [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j,\n",
      "          0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "        [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j,\n",
      "          0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "        [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j,\n",
      "         -0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "        [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j,\n",
      "         -0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]])\n",
      "torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "# To create rotators for \"cos(theta) + i*sin(theta)\"\n",
    "freqs_cos = emb.cos()  # real part\n",
    "freqs_sin = emb.sin()  # imaginery part\n",
    "freqs_cis = torch.complex(freqs_cos, freqs_sin)\n",
    "print(freqs_cis)\n",
    "print(freqs_cis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9fbc35",
   "metadata": {},
   "source": [
    "#### Defining Rotations Function - `apply_rotary_emb_torch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18b90584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb_torch(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    # making sure \"freqs_cis\" is on the right device\n",
    "    freqs_cis = freqs_cis.to(xq.device)\n",
    "\n",
    "    # intead of having all the frequencies, we filter out the ones for the correspondings tokens we need.  \n",
    "    freqs_cis = freqs_cis[position_ids]  # LINE 6\n",
    "\n",
    "    freqs_cis = freqs_cis[:, None, :, :]  # LINE 7\n",
    "\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # LINE 8\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]]\n",
    "\n",
    "    # Applying the rotations\n",
    "    xq_rotated = xq_ * freqs_cis_broadcast\n",
    "    xk_rotated = xk_ * freqs_cis_broadcast\n",
    "\n",
    "    # Convert back to real representatinos\n",
    "    # This flattens the last 2 dimensions back into one.\n",
    "    # input: [batch, num_heads, seq_length, head_dim / 2 (4), 2] (complex)\n",
    "    # output: [batch, num_heads, seq_length, 8]\n",
    "    xq_out = torch.view_as_real(xq_rotated).flatten(3) \n",
    "    xk_out = torch.view_as_real(xk_rotated).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924579e9",
   "metadata": {},
   "source": [
    "#### Breakdown of `apply_rotary_emb_torch()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cf114",
   "metadata": {},
   "source": [
    "LINE 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e60cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs_cis shape: torch.Size([10, 8])\n",
      "position_ids shape: torch.Size([2, 10])\n",
      "freqs_cis[position_ids] shape: torch.Size([2, 10, 8])\n",
      "final shape: \n",
      "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,\n",
      "          1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j,\n",
      "          0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "        [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j,\n",
      "         -0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "        [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j,\n",
      "         -0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "        [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j,\n",
      "         -0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "        [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j,\n",
      "          0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "        [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j,\n",
      "          0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "        [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j,\n",
      "          0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "        [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j,\n",
      "         -0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "        [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j,\n",
      "         -0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]])\n"
     ]
    }
   ],
   "source": [
    "print(f'freqs_cis shape: {freqs_cis.shape}')\n",
    "print(f'position_ids shape: {position_ids.shape}')\n",
    "print(f'freqs_cis[position_ids] shape: {freqs_cis[position_ids].shape}')\n",
    "print(f'final shape: \\n{freqs_cis[position_ids][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c19110",
   "metadata": {},
   "source": [
    "LINE 7\n",
    "  \n",
    "The Query and Key tensors have a shape of `[batch, num_heads, seq_len, head_dim]`. So, we add a dimension of `1` to `freqs_cis` to align with Q & K so we can broadcast them with this angles tensor.  \n",
    "\n",
    "Can use `unsqueeze(1)` too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d02ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8])\n",
      "torch.Size([2, 1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "freqs_cis = freqs_cis[position_ids]\n",
    "print(freqs_cis.shape)\n",
    "freqs_cis = freqs_cis[:, None, :, :]\n",
    "print(freqs_cis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d6cb3",
   "metadata": {},
   "source": [
    "LINE 8  \n",
    "\n",
    "Now, we reshapre the Query and Key tensors so that the pairs of numbers are treated as complex numbers **(a+ib)**.  \n",
    "*Same process for \"xk\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73c3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.8011, -0.5517,  1.2441,  ..., -0.8717, -0.2942,  0.3337],\n",
      "          [ 0.1762, -0.1274, -0.8008,  ..., -1.1549, -0.4158, -1.7951],\n",
      "          [-2.3783, -0.9613, -2.1015,  ...,  1.3007, -1.0858,  0.3678],\n",
      "          ...,\n",
      "          [-0.5342,  0.1280, -0.4700,  ...,  0.9925,  0.6600,  1.5047],\n",
      "          [ 0.6374,  1.0656,  1.0083,  ...,  1.0599,  1.4468, -0.8652],\n",
      "          [ 0.0680,  0.1107, -1.6959,  ...,  0.9235,  1.0702,  0.6899]],\n",
      "\n",
      "         [[ 1.5018,  0.9565, -1.0583,  ...,  0.8433, -0.3213,  1.2631],\n",
      "          [ 0.7975, -0.1302, -0.2862,  ...,  1.8829,  0.0470,  1.5126],\n",
      "          [-0.1961,  0.0146,  0.3894,  ..., -0.9640,  0.0291, -0.8071],\n",
      "          ...,\n",
      "          [-0.4551, -0.5375,  1.8866,  ...,  0.1935, -0.1347, -0.1596],\n",
      "          [ 1.0800, -2.1215, -1.3935,  ..., -0.1580, -0.9938, -1.1936],\n",
      "          [ 0.4865,  0.2801, -0.1601,  ...,  0.4223,  0.5414,  0.1586]],\n",
      "\n",
      "         [[ 0.5211,  0.2147,  1.4402,  ...,  1.8647, -1.8323,  1.3069],\n",
      "          [ 0.3001,  0.2398, -0.9430,  ..., -0.8664,  1.0388,  1.0751],\n",
      "          [-1.3928,  0.2338,  0.4019,  ...,  0.1353,  0.0426,  0.9569],\n",
      "          ...,\n",
      "          [ 1.5050, -0.1753,  0.4697,  ...,  1.2028,  1.0958,  0.6408],\n",
      "          [ 0.6259,  0.0310,  2.0470,  ...,  1.0573, -0.0413,  0.4832],\n",
      "          [-1.0886, -1.5154,  1.2020,  ...,  0.1804,  0.0945, -0.9067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2940,  0.0462, -0.6973,  ...,  0.9776,  0.3430, -1.6760],\n",
      "          [-0.0867, -0.9195,  0.6667,  ..., -3.4245,  0.2251, -0.2830],\n",
      "          [ 0.4394, -0.0074,  1.0045,  ..., -0.3994, -1.0095,  2.0196],\n",
      "          ...,\n",
      "          [ 1.2348, -2.0090, -0.2044,  ...,  1.6067, -0.4039, -0.1558],\n",
      "          [ 1.5133,  0.3667,  0.0079,  ...,  0.6463,  0.5581,  0.0629],\n",
      "          [ 0.4626,  0.2977,  1.3681,  ..., -0.4673, -1.9059,  0.5545]],\n",
      "\n",
      "         [[ 1.0854,  0.7462, -0.0888,  ..., -0.1235, -0.6239,  0.4802],\n",
      "          [ 1.1291, -0.0379,  0.4666,  ...,  0.7060, -0.2923, -0.5499],\n",
      "          [-0.8288,  1.3542, -0.3524,  ..., -0.5295, -1.3424, -1.1722],\n",
      "          ...,\n",
      "          [ 2.0386, -1.2172, -0.0626,  ...,  0.2755, -0.0870,  0.5118],\n",
      "          [ 1.2932, -1.3931, -0.6286,  ...,  0.8451, -1.6330,  0.2341],\n",
      "          [ 0.7883, -1.7499,  0.1999,  ..., -1.0476,  0.3095,  1.7801]],\n",
      "\n",
      "         [[ 0.0162,  0.0055,  0.6429,  ...,  0.3824, -1.3283,  1.4842],\n",
      "          [-1.6005,  2.1088, -0.5929,  ...,  0.5081,  0.7192, -0.2037],\n",
      "          [ 0.5617, -1.8397, -1.4682,  ..., -1.4761, -1.0822, -0.2580],\n",
      "          ...,\n",
      "          [ 0.9833,  0.2792, -1.8523,  ..., -0.7130,  0.2385,  0.1781],\n",
      "          [ 0.8940,  1.3886,  1.3564,  ..., -0.3522, -2.4223, -1.1826],\n",
      "          [-1.0077,  1.3152,  1.0974,  ...,  0.7682, -0.7211, -1.5168]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5922, -0.6350,  0.4461,  ...,  0.6067, -1.7861, -1.8030],\n",
      "          [-1.3734,  2.0454, -0.1976,  ...,  1.1355, -0.0404, -1.3656],\n",
      "          [ 0.9154,  1.1340,  0.5651,  ..., -1.1531,  0.5761,  0.9172],\n",
      "          ...,\n",
      "          [-1.4368,  1.2994, -1.3753,  ...,  2.0309, -0.6752,  1.4835],\n",
      "          [ 0.4674, -1.2331,  1.6251,  ...,  0.6961,  0.4998, -0.0837],\n",
      "          [-0.4126,  0.4584,  1.1674,  ...,  0.0532,  2.7045,  1.1794]],\n",
      "\n",
      "         [[-0.1506, -0.6920, -1.1618,  ...,  0.8197,  0.3263, -0.4676],\n",
      "          [ 0.2625,  2.2612,  1.1514,  ..., -0.0615, -2.3085, -0.8576],\n",
      "          [-1.0555, -0.9648, -0.0151,  ..., -0.8415,  0.6684, -0.5485],\n",
      "          ...,\n",
      "          [-2.0134, -1.2888, -1.3781,  ...,  0.1670,  0.1205,  1.1772],\n",
      "          [ 0.2083,  0.0145, -2.4004,  ..., -0.8994,  0.1144,  0.1177],\n",
      "          [-0.0492, -2.4687, -0.9145,  ..., -0.4519,  0.2912,  0.3972]],\n",
      "\n",
      "         [[ 0.1735,  1.4142, -1.1889,  ..., -0.5126,  1.2230, -0.1668],\n",
      "          [ 0.1898,  0.3021,  1.0851,  ...,  2.1549,  0.5525,  0.0098],\n",
      "          [ 0.9288,  1.4002,  1.7312,  ...,  1.9116, -0.3867,  1.8782],\n",
      "          ...,\n",
      "          [-0.0077, -0.6147, -0.2113,  ..., -0.2046, -0.6037, -1.1615],\n",
      "          [-1.0848,  1.3818, -0.5008,  ...,  2.2552, -1.7400,  1.3897],\n",
      "          [ 0.1582,  2.8404, -0.7392,  ...,  0.9959, -0.3989,  0.8601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4464,  0.3756,  1.5236,  ..., -0.2831, -0.1253,  0.5725],\n",
      "          [ 0.7072, -0.5884, -1.6144,  ..., -1.7085,  0.2312, -0.8767],\n",
      "          [ 0.8969,  0.6809, -0.0917,  ...,  1.1542, -0.2700,  0.7507],\n",
      "          ...,\n",
      "          [ 1.3691, -0.6322,  0.5203,  ...,  1.2622, -0.6691,  1.3126],\n",
      "          [-0.0443, -1.4293, -0.3599,  ...,  1.0354,  0.7702,  0.2818],\n",
      "          [-0.1565, -0.6232,  0.6213,  ...,  0.0395,  0.7935, -0.5916]],\n",
      "\n",
      "         [[-0.3095,  1.4352, -0.0814,  ...,  0.6601,  0.2587, -1.9377],\n",
      "          [-0.2076, -1.2934, -0.4961,  ..., -1.5821,  0.8282,  0.1264],\n",
      "          [ 0.6118,  0.2429, -0.4984,  ..., -0.2268, -0.9570, -1.1678],\n",
      "          ...,\n",
      "          [ 0.3962, -0.9689, -0.3834,  ..., -0.6925,  0.6691, -0.3638],\n",
      "          [-2.5463,  0.2418,  2.1431,  ..., -0.3108, -1.2682, -1.8996],\n",
      "          [-1.2378,  0.4145, -0.3340,  ..., -1.3132,  0.6186,  0.7881]],\n",
      "\n",
      "         [[ 0.1627, -0.9479,  0.2914,  ...,  0.4107, -1.6072, -0.5889],\n",
      "          [-0.1229,  1.2364,  0.5439,  ...,  0.5870,  0.0140, -0.2195],\n",
      "          [-1.8897,  1.0495,  0.4650,  ...,  1.5156, -0.6029,  0.8890],\n",
      "          ...,\n",
      "          [-0.0067,  0.6587, -1.5780,  ...,  1.1813, -0.2504, -0.8904],\n",
      "          [ 1.0538,  0.9510, -0.2236,  ...,  0.0054, -2.1030, -0.6985],\n",
      "          [ 0.7926,  0.9932,  0.5485,  ...,  0.2563, -0.4201, -0.5072]]]])\n",
      "torch.Size([2, 16, 10, 8])\n",
      "\n",
      "tensor([[[[ 0.8011, -0.5517,  1.2441,  ..., -0.8717, -0.2942,  0.3337],\n",
      "          [ 0.1762, -0.1274, -0.8008,  ..., -1.1549, -0.4158, -1.7951],\n",
      "          [-2.3783, -0.9613, -2.1015,  ...,  1.3007, -1.0858,  0.3678],\n",
      "          ...,\n",
      "          [-0.5342,  0.1280, -0.4700,  ...,  0.9925,  0.6600,  1.5047],\n",
      "          [ 0.6374,  1.0656,  1.0083,  ...,  1.0599,  1.4468, -0.8652],\n",
      "          [ 0.0680,  0.1107, -1.6959,  ...,  0.9235,  1.0702,  0.6899]],\n",
      "\n",
      "         [[ 1.5018,  0.9565, -1.0583,  ...,  0.8433, -0.3213,  1.2631],\n",
      "          [ 0.7975, -0.1302, -0.2862,  ...,  1.8829,  0.0470,  1.5126],\n",
      "          [-0.1961,  0.0146,  0.3894,  ..., -0.9640,  0.0291, -0.8071],\n",
      "          ...,\n",
      "          [-0.4551, -0.5375,  1.8866,  ...,  0.1935, -0.1347, -0.1596],\n",
      "          [ 1.0800, -2.1215, -1.3935,  ..., -0.1580, -0.9938, -1.1936],\n",
      "          [ 0.4865,  0.2801, -0.1601,  ...,  0.4223,  0.5414,  0.1586]],\n",
      "\n",
      "         [[ 0.5211,  0.2147,  1.4402,  ...,  1.8647, -1.8323,  1.3069],\n",
      "          [ 0.3001,  0.2398, -0.9430,  ..., -0.8664,  1.0388,  1.0751],\n",
      "          [-1.3928,  0.2338,  0.4019,  ...,  0.1353,  0.0426,  0.9569],\n",
      "          ...,\n",
      "          [ 1.5050, -0.1753,  0.4697,  ...,  1.2028,  1.0958,  0.6408],\n",
      "          [ 0.6259,  0.0310,  2.0470,  ...,  1.0573, -0.0413,  0.4832],\n",
      "          [-1.0886, -1.5154,  1.2020,  ...,  0.1804,  0.0945, -0.9067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2940,  0.0462, -0.6973,  ...,  0.9776,  0.3430, -1.6760],\n",
      "          [-0.0867, -0.9195,  0.6667,  ..., -3.4245,  0.2251, -0.2830],\n",
      "          [ 0.4394, -0.0074,  1.0045,  ..., -0.3994, -1.0095,  2.0196],\n",
      "          ...,\n",
      "          [ 1.2348, -2.0090, -0.2044,  ...,  1.6067, -0.4039, -0.1558],\n",
      "          [ 1.5133,  0.3667,  0.0079,  ...,  0.6463,  0.5581,  0.0629],\n",
      "          [ 0.4626,  0.2977,  1.3681,  ..., -0.4673, -1.9059,  0.5545]],\n",
      "\n",
      "         [[ 1.0854,  0.7462, -0.0888,  ..., -0.1235, -0.6239,  0.4802],\n",
      "          [ 1.1291, -0.0379,  0.4666,  ...,  0.7060, -0.2923, -0.5499],\n",
      "          [-0.8288,  1.3542, -0.3524,  ..., -0.5295, -1.3424, -1.1722],\n",
      "          ...,\n",
      "          [ 2.0386, -1.2172, -0.0626,  ...,  0.2755, -0.0870,  0.5118],\n",
      "          [ 1.2932, -1.3931, -0.6286,  ...,  0.8451, -1.6330,  0.2341],\n",
      "          [ 0.7883, -1.7499,  0.1999,  ..., -1.0476,  0.3095,  1.7801]],\n",
      "\n",
      "         [[ 0.0162,  0.0055,  0.6429,  ...,  0.3824, -1.3283,  1.4842],\n",
      "          [-1.6005,  2.1088, -0.5929,  ...,  0.5081,  0.7192, -0.2037],\n",
      "          [ 0.5617, -1.8397, -1.4682,  ..., -1.4761, -1.0822, -0.2580],\n",
      "          ...,\n",
      "          [ 0.9833,  0.2792, -1.8523,  ..., -0.7130,  0.2385,  0.1781],\n",
      "          [ 0.8940,  1.3886,  1.3564,  ..., -0.3522, -2.4223, -1.1826],\n",
      "          [-1.0077,  1.3152,  1.0974,  ...,  0.7682, -0.7211, -1.5168]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5922, -0.6350,  0.4461,  ...,  0.6067, -1.7861, -1.8030],\n",
      "          [-1.3734,  2.0454, -0.1976,  ...,  1.1355, -0.0404, -1.3656],\n",
      "          [ 0.9154,  1.1340,  0.5651,  ..., -1.1531,  0.5761,  0.9172],\n",
      "          ...,\n",
      "          [-1.4368,  1.2994, -1.3753,  ...,  2.0309, -0.6752,  1.4835],\n",
      "          [ 0.4674, -1.2331,  1.6251,  ...,  0.6961,  0.4998, -0.0837],\n",
      "          [-0.4126,  0.4584,  1.1674,  ...,  0.0532,  2.7045,  1.1794]],\n",
      "\n",
      "         [[-0.1506, -0.6920, -1.1618,  ...,  0.8197,  0.3263, -0.4676],\n",
      "          [ 0.2625,  2.2612,  1.1514,  ..., -0.0615, -2.3085, -0.8576],\n",
      "          [-1.0555, -0.9648, -0.0151,  ..., -0.8415,  0.6684, -0.5485],\n",
      "          ...,\n",
      "          [-2.0134, -1.2888, -1.3781,  ...,  0.1670,  0.1205,  1.1772],\n",
      "          [ 0.2083,  0.0145, -2.4004,  ..., -0.8994,  0.1144,  0.1177],\n",
      "          [-0.0492, -2.4687, -0.9145,  ..., -0.4519,  0.2912,  0.3972]],\n",
      "\n",
      "         [[ 0.1735,  1.4142, -1.1889,  ..., -0.5126,  1.2230, -0.1668],\n",
      "          [ 0.1898,  0.3021,  1.0851,  ...,  2.1549,  0.5525,  0.0098],\n",
      "          [ 0.9288,  1.4002,  1.7312,  ...,  1.9116, -0.3867,  1.8782],\n",
      "          ...,\n",
      "          [-0.0077, -0.6147, -0.2113,  ..., -0.2046, -0.6037, -1.1615],\n",
      "          [-1.0848,  1.3818, -0.5008,  ...,  2.2552, -1.7400,  1.3897],\n",
      "          [ 0.1582,  2.8404, -0.7392,  ...,  0.9959, -0.3989,  0.8601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4464,  0.3756,  1.5236,  ..., -0.2831, -0.1253,  0.5725],\n",
      "          [ 0.7072, -0.5884, -1.6144,  ..., -1.7085,  0.2312, -0.8767],\n",
      "          [ 0.8969,  0.6809, -0.0917,  ...,  1.1542, -0.2700,  0.7507],\n",
      "          ...,\n",
      "          [ 1.3691, -0.6322,  0.5203,  ...,  1.2622, -0.6691,  1.3126],\n",
      "          [-0.0443, -1.4293, -0.3599,  ...,  1.0354,  0.7702,  0.2818],\n",
      "          [-0.1565, -0.6232,  0.6213,  ...,  0.0395,  0.7935, -0.5916]],\n",
      "\n",
      "         [[-0.3095,  1.4352, -0.0814,  ...,  0.6601,  0.2587, -1.9377],\n",
      "          [-0.2076, -1.2934, -0.4961,  ..., -1.5821,  0.8282,  0.1264],\n",
      "          [ 0.6118,  0.2429, -0.4984,  ..., -0.2268, -0.9570, -1.1678],\n",
      "          ...,\n",
      "          [ 0.3962, -0.9689, -0.3834,  ..., -0.6925,  0.6691, -0.3638],\n",
      "          [-2.5463,  0.2418,  2.1431,  ..., -0.3108, -1.2682, -1.8996],\n",
      "          [-1.2378,  0.4145, -0.3340,  ..., -1.3132,  0.6186,  0.7881]],\n",
      "\n",
      "         [[ 0.1627, -0.9479,  0.2914,  ...,  0.4107, -1.6072, -0.5889],\n",
      "          [-0.1229,  1.2364,  0.5439,  ...,  0.5870,  0.0140, -0.2195],\n",
      "          [-1.8897,  1.0495,  0.4650,  ...,  1.5156, -0.6029,  0.8890],\n",
      "          ...,\n",
      "          [-0.0067,  0.6587, -1.5780,  ...,  1.1813, -0.2504, -0.8904],\n",
      "          [ 1.0538,  0.9510, -0.2236,  ...,  0.0054, -2.1030, -0.6985],\n",
      "          [ 0.7926,  0.9932,  0.5485,  ...,  0.2563, -0.4201, -0.5072]]]])\n",
      "torch.Size([2, 16, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# random xq for testing\n",
    "xq = torch.randn(batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "xk = torch.randn(batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "print(xq)\n",
    "print(f'{xq.shape}\\n')\n",
    "\n",
    "\n",
    "xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # keeps dimensions execpt the last one same, then splits the last one into two\n",
    "print(xq)\n",
    "print(xq.shape)\n",
    "\n",
    "xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))  # keeps dimensions execpt the last one same, then splits the last one into two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9c6fc",
   "metadata": {},
   "source": [
    "*8 has changed to 4 because we divided the deimension into pairs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30322f",
   "metadata": {},
   "source": [
    "LINE 9  \n",
    "\n",
    "Previsously, we calculated the `freqs_cis` tensor by using `head_dim` sines and cosines. However, xq's last dimension is now 4 (*Because each pair of numbers become a single complex number*).  \n",
    "To multiply them, we need freq_cis in the same dimesnions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f302e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially: torch.Size([2, 1, 10, 8])\n",
      "tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "          [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "          [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "          [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "          [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "          [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "          [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "          [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "          [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "          [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "          [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "          [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "          [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "          [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j],\n",
      "          [ 0.2837-0.9589j,  0.8776+0.4794j,  0.9988+0.0500j,  1.0000+0.0050j],\n",
      "          [ 0.9602-0.2794j,  0.8253+0.5646j,  0.9982+0.0600j,  1.0000+0.0060j],\n",
      "          [ 0.7539+0.6570j,  0.7648+0.6442j,  0.9976+0.0699j,  1.0000+0.0070j],\n",
      "          [-0.1455+0.9894j,  0.6967+0.7174j,  0.9968+0.0799j,  1.0000+0.0080j],\n",
      "          [-0.9111+0.4121j,  0.6216+0.7833j,  0.9960+0.0899j,  1.0000+0.0090j]]]])\n",
      "torch.Size([2, 1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "print(f'initially: {freqs_cis.shape}')\n",
    "\n",
    "freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]]\n",
    "print(freqs_cis_broadcast)\n",
    "print(freqs_cis_broadcast.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60bb02",
   "metadata": {},
   "source": [
    "Now, we have the rotations for 4 pairs in each token.  \n",
    "\n",
    "**NOTE:** *The slicing step is necessary because we duplicated the frequencies in the `simple_rope_calculation()` function. We do this to understand the existing implementation of transformers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d0c59",
   "metadata": {},
   "source": [
    "#### Calculating RoPE Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aad3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated freqs_cis shape: torch.Size([256, 8])\n"
     ]
    }
   ],
   "source": [
    "# pre-computed frequencies depending solely on the position, and not the token.\n",
    "freqs_cis = simple_rope_calculation(head_dim, max_positional_embeddings, base=rope_theta, device=hidden_states.device)\n",
    "print(f'Calculated freqs_cis shape: {freqs_cis.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107c8b2",
   "metadata": {},
   "source": [
    "#### Applying RoPE Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2392bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after RoPE:\n",
      "  query_states_rope: torch.Size([2, 16, 10, 8])\n",
      "  key_states_rope: torch.Size([2, 4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, freqs_cis)\n",
    "\n",
    "print('Shapes after RoPE:')\n",
    "print(f'  query_states_rope: {query_states_rope.shape}')\n",
    "print(f'  key_states_rope: {key_states_rope.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26e638",
   "metadata": {},
   "source": [
    "# Optional L2 Normalization  \n",
    "Normalization can sometimes help stabilize the model, espeically for deep neural networks,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d68a9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleL2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps # to avoid div by 0 during normalization, we use a v small eps value\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dd54679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query & Key states normalised.\n"
     ]
    }
   ],
   "source": [
    "if use_qk_norm:\n",
    "    qk_norm = SimpleL2Norm()\n",
    "    query_states_final = qk_norm(query_states_rope)\n",
    "    key_states_final = qk_norm(key_states_rope)\n",
    "    print('Query & Key states normalised.')\n",
    "else:\n",
    "    query_states_final = query_states_rope\n",
    "    key_states_final = key_states_rope\n",
    "    print('Query & Key states normalization skipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a04a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before Attention scores are calculated: \n",
      "query_states_final: torch.Size([2, 16, 10, 8])\n",
      "key_states_final: torch.Size([2, 4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print('Shapes before Attention scores are calculated: ')\n",
    "print(f'query_states_final: {query_states_final.shape}')\n",
    "print(f'key_states_final: {key_states_final.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c242a",
   "metadata": {},
   "source": [
    "# Grouped Query Attention (GQA)\n",
    "Since we have fewer K and V heads than Q heads, we need to repeat the K & V heads to match the number of Q heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437aee7",
   "metadata": {},
   "source": [
    "#### Defining Function to repeat the num of attention heads - `Repeat_kv()`}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36439bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, seq_len, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    \n",
    "    # inserting the new dimension\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, seq_len, head_dim)\n",
    "\n",
    "    # replicating the num_heads dimesnsion\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34073b7",
   "metadata": {},
   "source": [
    "#### Applying the function on K & V states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fab37a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after repeating K/V for GQA: \n",
      "key_states_repeated: torch.Size([2, 16, 10, 8])\n",
      "value_states_repeated: torch.Size([2, 16, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "key_states_repeated = repeat_kv(key_states_final, num_key_value_groups)\n",
    "value_states_repeated = repeat_kv(value_states, num_key_value_groups)\n",
    "\n",
    "print('Shapes after repeating K/V for GQA: ')\n",
    "print(f'key_states_repeated: {key_states_repeated.shape}')\n",
    "print(f'value_states_repeated: {value_states_repeated.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0874e7",
   "metadata": {},
   "source": [
    "# Attention Scores\n",
    "Attention scores(q, k, v) = softmax((Q * K^t) / sqrt(dim_of_k/v)) * v  \n",
    "\n",
    "key -> key^t :  \n",
    "`(batch, num_attn_heads, seq_len, head_dim) -> (batch, num_attn_heads, head_dim, seq_len)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3df4b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74831c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling factor: 0.35355339059327373\n",
      "tensor([[[[-0.8114, -1.1454,  0.2217,  ..., -0.3674,  0.6383, -1.7795],\n",
      "          [-1.6664,  1.1880, -0.4083,  ..., -1.4549, -0.9255,  0.4180],\n",
      "          [-0.4410, -2.3622, -1.4458,  ..., -1.3145,  1.7512, -1.8951],\n",
      "          ...,\n",
      "          [-0.9147, -1.7392,  1.1622,  ..., -0.6501,  0.2714, -0.0845],\n",
      "          [-1.3295,  1.2392,  0.4225,  ..., -1.0408, -0.8553,  1.8397],\n",
      "          [ 0.3062,  2.0284, -1.2780,  ...,  0.3445, -0.5611,  1.3786]],\n",
      "\n",
      "         [[-0.2460, -1.8457, -1.1507,  ..., -0.7210,  1.1247, -2.4328],\n",
      "          [-1.9595, -0.8400,  0.3843,  ..., -1.6351,  0.8147, -0.9490],\n",
      "          [ 0.7661,  0.9593, -2.0738,  ...,  0.3130,  0.8860,  0.0091],\n",
      "          ...,\n",
      "          [ 0.1281,  1.3787,  1.5253,  ...,  0.9533,  0.2300,  1.4844],\n",
      "          [ 0.3781,  0.0152,  1.5292,  ..., -0.0233, -1.1693,  0.5302],\n",
      "          [-0.5878,  1.4628,  0.3449,  ..., -0.6731, -1.5125,  1.9564]],\n",
      "\n",
      "         [[-0.2809,  0.8520,  0.4824,  ..., -0.9812, -2.4452,  1.1108],\n",
      "          [-0.2836, -1.0224,  1.5185,  ...,  0.6700,  1.2638, -0.9796],\n",
      "          [-0.5592,  0.0278,  0.5208,  ...,  0.4130, -0.8699, -0.1407],\n",
      "          ...,\n",
      "          [-0.0358,  0.5378,  0.1621,  ...,  0.2320,  1.0124, -0.3655],\n",
      "          [ 0.8300, -0.0596,  0.3876,  ...,  1.2929,  0.7334, -1.0135],\n",
      "          [-0.1144,  1.1449, -0.7648,  ..., -0.7042, -0.8587,  0.1099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9759, -0.9622,  0.9310,  ...,  0.6557,  2.0176, -0.4486],\n",
      "          [-0.6997, -2.2546,  1.5562,  ..., -0.4671,  0.9254,  1.5129],\n",
      "          [ 0.8871,  0.0316, -0.3067,  ..., -0.8013, -1.4093,  0.9581],\n",
      "          ...,\n",
      "          [-0.3866, -1.4033,  0.1105,  ..., -0.5042,  2.2684,  0.7862],\n",
      "          [ 1.7254,  1.5839,  0.5646,  ..., -0.6124, -0.0355, -0.0706],\n",
      "          [ 1.6329,  0.8765, -1.6351,  ...,  0.2844, -0.3969, -0.2378]],\n",
      "\n",
      "         [[-0.2287, -0.5939, -0.5989,  ...,  1.3703,  1.0746,  0.0464],\n",
      "          [-1.8607, -1.7723, -0.9981,  ...,  1.3126,  0.5980,  0.9792],\n",
      "          [ 0.7866,  2.3716, -1.0146,  ..., -0.0218, -2.0673, -1.3360],\n",
      "          ...,\n",
      "          [-1.9161, -1.5555,  1.4247,  ...,  0.0438,  0.2575, -0.1060],\n",
      "          [-0.3765,  0.8012,  1.1828,  ..., -0.0791,  0.4832, -1.1929],\n",
      "          [-0.5061, -1.1507,  1.5335,  ...,  0.4250,  0.3543,  0.6188]],\n",
      "\n",
      "         [[-2.0325, -0.2445,  0.5347,  ...,  1.7092,  0.1523, -0.8651],\n",
      "          [-1.0901, -1.8616, -0.4177,  ...,  0.3574,  1.2754,  2.1088],\n",
      "          [ 1.1880,  2.0229, -0.7746,  ...,  0.3870, -2.5278, -1.7327],\n",
      "          ...,\n",
      "          [ 2.0400,  0.3525, -0.5395,  ..., -0.6172, -0.2900,  0.8739],\n",
      "          [ 0.1965, -0.6239,  1.8933,  ..., -0.7260,  1.8504,  0.0252],\n",
      "          [ 1.4634, -0.4339,  0.8354,  ..., -1.2260, -0.9873,  0.8908]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3598,  0.3263, -0.2982,  ...,  1.7676,  0.9224, -0.9007],\n",
      "          [-0.1162,  1.3365, -0.1583,  ...,  0.3416,  1.1056, -1.9653],\n",
      "          [ 1.9054, -1.5525, -0.1182,  ..., -2.0572, -0.5169, -0.1069],\n",
      "          ...,\n",
      "          [-1.5813, -0.5243, -1.2826,  ..., -0.3721, -0.5817,  2.0960],\n",
      "          [-0.4498,  1.5480,  1.1116,  ...,  0.0157,  0.9412, -2.0497],\n",
      "          [ 0.8196, -1.9086, -0.8210,  ...,  0.0646, -1.5515,  1.6846]],\n",
      "\n",
      "         [[-0.3764,  1.3736,  1.3245,  ..., -0.5594, -0.2661,  0.7529],\n",
      "          [-0.0442,  1.7100,  0.9161,  ...,  1.0090, -0.4283, -0.5433],\n",
      "          [ 0.1911, -2.2307, -1.2789,  ..., -0.8951, -1.1844,  0.8457],\n",
      "          ...,\n",
      "          [ 0.7928,  0.6733,  1.2643,  ...,  0.3788,  1.3976, -0.0215],\n",
      "          [-1.8183,  0.3329, -1.3314,  ...,  0.3249, -0.5443,  0.0428],\n",
      "          [ 0.0689,  1.7716,  0.6555,  ..., -0.5909,  0.2127, -1.9652]],\n",
      "\n",
      "         [[-0.7504,  0.1308,  0.7913,  ...,  0.5446, -0.1818,  1.6946],\n",
      "          [-0.7700,  0.2456, -1.1119,  ..., -0.6916,  1.4923, -0.5143],\n",
      "          [ 0.3071,  1.0441,  0.1261,  ..., -0.3543, -0.2661, -0.0136],\n",
      "          ...,\n",
      "          [-1.7906,  0.0079, -0.8470,  ..., -0.0504, -0.5396,  0.0635],\n",
      "          [ 0.4773, -1.1605, -0.7197,  ..., -1.5746,  0.0602, -0.8821],\n",
      "          [-1.3440,  0.9553, -0.0515,  ..., -0.8989, -0.7705,  1.0703]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5599,  1.1399, -0.1042,  ..., -0.6319, -1.3784, -1.2202],\n",
      "          [-0.4356, -0.5859,  0.1249,  ...,  0.9509, -0.1693,  1.7270],\n",
      "          [ 1.1954, -0.7113, -0.9948,  ...,  1.4321, -0.2271, -0.3133],\n",
      "          ...,\n",
      "          [ 1.3487,  0.7126, -1.2126,  ...,  0.6485, -0.0970,  0.3935],\n",
      "          [ 0.7624,  0.9751,  1.6098,  ..., -1.9130,  0.3566, -1.2028],\n",
      "          [-0.5112, -0.8520,  1.5280,  ...,  0.2392, -1.0807,  1.8358]],\n",
      "\n",
      "         [[-0.1528, -2.3403,  0.6920,  ...,  1.0290, -0.8517,  2.1865],\n",
      "          [ 1.0639,  0.5368,  0.0142,  ...,  0.0565, -0.2347, -0.2399],\n",
      "          [-0.2562, -1.4630,  0.3698,  ...,  1.2935,  0.7428,  1.2241],\n",
      "          ...,\n",
      "          [ 1.6844,  1.6434,  0.7345,  ..., -0.5233,  0.4973, -1.0275],\n",
      "          [-0.5467, -0.9023,  0.3402,  ..., -1.1570, -1.5046, -0.1702],\n",
      "          [ 0.6035,  0.1589,  1.8761,  ..., -0.9790, -0.5328, -0.9275]],\n",
      "\n",
      "         [[ 0.1556,  0.3866, -0.2643,  ...,  0.4703,  1.9282,  1.3751],\n",
      "          [-0.4863, -1.2852,  0.3795,  ..., -0.6530, -0.5915,  1.2008],\n",
      "          [ 0.5872, -0.4336, -1.2215,  ...,  0.9611, -1.5234, -1.1578],\n",
      "          ...,\n",
      "          [-0.8799,  0.2238,  1.6345,  ..., -1.4713,  0.5909,  0.5209],\n",
      "          [-0.7928, -0.9525, -0.4820,  ...,  0.0740, -0.5122,  1.2120],\n",
      "          [ 0.1979,  0.7409,  1.0555,  ..., -0.6082, -0.7774, -0.6683]]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# scaling\n",
    "scaling_factor = 1.0/math.sqrt(head_dim)\n",
    "attn_weights = attn_weights * scaling_factor # scaled attn weights\n",
    "\n",
    "print(f'scaling factor: {scaling_factor}')\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e14de0",
   "metadata": {},
   "source": [
    "We will apply the attention_mask to the attn_weights to cover the tokens that comes after the current token, this will be called causal mask.  \n",
    "*Also, to make sure that they're in the same shape*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d916aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying attention mask with shape: torch.Size([2, 1, 10, 10])\n",
      "\n",
      "Shape of causal mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Apply the mask\n",
    "if attention_mask is not None:\n",
    "    print(f'Applying attention mask with shape: {attention_mask.shape}')\n",
    "\n",
    "    # Already have the same shape in this notebook but just to be sure\n",
    "    causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]]\n",
    "    print(f'\\nShape of causal mask: {causal_mask.shape}')\n",
    "\n",
    "    # adding the mask over the attn_weights\n",
    "    attn_weights = attn_weights + causal_mask\n",
    "else:\n",
    "    print('\\nNo Attention mask Applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908daad5",
   "metadata": {},
   "source": [
    "shape of `attn_weights`: `[batch_size, num_attention_heads, Q_sequence_length, K_sequence_length]`  \n",
    "\n",
    "`dim=-1` tells the function to calcualte the softmax probabilities w.r.t the Keys so the query will know how much attention to give each token acc to the attention score assigned to each token based on its key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72d57a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
    "\n",
    "print(attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abccf8",
   "metadata": {},
   "source": [
    "#### Dropouts\n",
    "Basically used to randomly set some attn_weights to 0 to avoid overfitting.  \n",
    "But, we skip it for simplicity\n",
    "\n",
    "*   `p=attention_dropout` is set to 0, it's the probability of an element being zeroed out.\n",
    "*   `training = self.training` tells it to be active only while training, and during eval mode (`model.eval()`) it is set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bc57525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_weights = nn.functional.dropout(attn_weights, p=attention_dropout, training = self.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055b722",
   "metadata": {},
   "source": [
    "#### Final Attention Value-states Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fd187cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = torch.matmul(attn_weights, value_states_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d2e5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention caclulatin Shapes:\n",
      "    attn_weights (raw scores): torch.Size([2, 16, 10, 10])\n",
      "    attn_weights (after softmax): torch.Size([2, 16, 10, 10])\n",
      "    attn_output: torch.Size([2, 16, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print('\\nAttention caclulatin Shapes:')\n",
    "print(f'    attn_weights (raw scores): {attn_weights.shape}')\n",
    "print(f'    attn_weights (after softmax): {attn_weights.shape}')\n",
    "print(f'    attn_output: {attn_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e8965",
   "metadata": {},
   "source": [
    "# Reshaping Output Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab24ae",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "**Memory Layout:** After the `.transpose()` and `.contiguous()` operations, your `attn_output` tensor of shape `[2, 10, 16, 8]` is stored in memory as a single, continuous block of `2 * 10 * 16 * 8 = 2560` numbers.\n",
    "\n",
    "**The view Command:** When you call `.view(2, 10, 128)`, you are giving PyTorch a new blueprint. It takes the flat list of 2560 numbers and starts filling in this new 3D shape:\n",
    "\n",
    "*   It creates the first dimension of size 2.\n",
    "\n",
    "*   Inside that, it creates the second dimension of size 10.\n",
    "\n",
    "*   It then takes the remaining 128 numbers (`16 * 8`) needed to fill the last dimension for each of the `2x10` entries.\n",
    "\n",
    "Because of the order of the dimensions after the transpose (`[batch, seq_len, heads, head_dim]`), the `heads` and `head_dim` dimensions are next to each other in memory. The `.view()` operation naturally groups them together to form the new `hidden_size` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bdd619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 16, 8])\n",
      "torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# (batch, num_attn_heads, seq_len, head_dim) -> (batch, seq_len, num_attn_heads, head_dim)\n",
    "attn_output = attn_output.transpose(1,2).contiguous()\n",
    "print(attn_output.shape)\n",
    "\n",
    "\n",
    "# -> (batch, seq_len, num_attn_heads * head_dim) = (batch, seq_len, hidden_size)\n",
    "attn_output = attn_output.view(batch_size, sequence_length, hidden_size)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24f23d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shapes:\n",
      "    attn_output (reshaped): torch.Size([2, 10, 128])\n",
      "    final_attn_output: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# prijecting it through the LinearLayer we previously created\n",
    "final_attn_output = o_proj(attn_output)\n",
    "\n",
    "print('Final Output Shapes:')\n",
    "print(f'    attn_output (reshaped): {attn_output.shape}')\n",
    "print(f'    final_attn_output: {final_attn_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edd358",
   "metadata": {},
   "source": [
    "# Combining it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3715ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape from simplified attention module: torch.Size([2, 10, 128])\n",
      "Attention weights shape from simplified attention moduel: torch.Size([2, 16, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class SimplifiedLlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_attention_heads = config['num_attention_heads']\n",
    "        self.num_key_value_heads = config['num_key_value_heads']\n",
    "        self.head_dim = self.hidden_size // self.num_attention_heads\n",
    "        self.num_key_value_groups  =self.num_attention_heads // self.num_key_value_heads\n",
    "        self.max_positional_embeddings = config['max_positional_embeddings']\n",
    "        self.rope_theta = config['rope_theta']\n",
    "        self.attention_bias = config['attention_bias']\n",
    "        self.use_qk_norm = config['use_qk_norm']\n",
    "\n",
    "        if (self.head_dim * self.num_attention_heads) != self.hidden_size:\n",
    "            raise ValueError(\"hidden_size must be divisible by num_attention_heads\")\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=self.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=self.attention_bias)\n",
    "\n",
    "        self.freqs_cis = simple_rope_calculation(self.head_dim, self.max_positional_embeddings, base=self.rope_theta)\n",
    "        if self.use_qk_norm:\n",
    "            self.qk_norm = SimpleL2Norm()\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, position_ids):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "\n",
    "        # projections\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        #reshape\n",
    "        query_states = query_states.view(batch_size, sequence_length, self.num_attention_heads, self.head_dim).transpose(1,2)\n",
    "        key_states = key_states.view(batch_size, sequence_length, self.num_key_value_heads, self.head_dim).transpose(1,2)\n",
    "        value_states = value_states.view(batch_size, sequence_length, self.num_key_value_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # apply rope\n",
    "        current_freqs_cis = self.freqs_cis.to(hidden_states.device) # getting precomputed freqs on device\n",
    "        query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, current_freqs_cis)\n",
    "\n",
    "        # optional QK norm\n",
    "        if self.use_qk_norm:\n",
    "            query_states_final = self.qk_norm(query_states_rope)\n",
    "            key_states_final = self.qk_norm(key_states_rope)\n",
    "        else:\n",
    "            query_states_final = query_states_rope\n",
    "            key_states_final = key_states_rope\n",
    "\n",
    "        # repeater for K/V (GQA)\n",
    "        key_states_repeated = repeat_kv(key_states_final, self.num_key_value_groups)\n",
    "        value_states_repeated = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        # attention calculation\n",
    "        attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2,3))\n",
    "        scaling_factor = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_weights = attn_weights * scaling_factor\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
    "\n",
    "        # can add dropout if you want\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states_repeated)\n",
    "\n",
    "        # reshape and output projection layer\n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, sequence_length, self.hidden_size)\n",
    "        final_attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return final_attn_output, attn_weights  # returning weight is optional\n",
    "    \n",
    "\n",
    "# Instantiating the simplified model\n",
    "config_dict = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_attention_heads': num_attention_heads,\n",
    "    'num_key_value_heads': num_key_value_heads,\n",
    "    'max_positional_embeddings': max_positional_embeddings,\n",
    "    'rope_theta': rope_theta,\n",
    "    'attention_bias': attention_bias,\n",
    "    'use_qk_norm': use_qk_norm\n",
    "}\n",
    "\n",
    "simplified_attn_module = SimplifiedLlamaAttention(config_dict)\n",
    "\n",
    "# forward pass\n",
    "final_output_simplified, final_weights_simplified = simplified_attn_module(hidden_states, attention_mask, position_ids)\n",
    "\n",
    "print(f\"\\nOutput shape from simplified attention module: {final_output_simplified.shape}\")\n",
    "print(f'Attention weights shape from simplified attention moduel: {final_weights_simplified.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4582d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
