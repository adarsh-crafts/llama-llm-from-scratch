{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948ca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Coding\\Python\\llama-from-scratch\\venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75108c81",
   "metadata": {},
   "source": [
    "# Config settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5378bf1",
   "metadata": {},
   "source": [
    "#### Architecural Dimensions\n",
    "$\\underline{\\text{Grouped-Query Attention (GQA)}}$  \n",
    "This is a technique in which we use fewer number of Key/Value heads than the Query heads.  This method requires significantly less memory, and can generate text much faster with a very small impact on the overall accuracy.  \n",
    "> __note:__ The number of Query heads must be perfectly divisble by the number of Key/Value heads.  \n",
    "\n",
    "*In this case, we use one Key and Value heads per 4 Query heads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fa0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 128   # synonymous with embeddings dimension\n",
    "num_attention_heads = 16    # The no. of attention query heads\n",
    "num_key_value_heads = 4     # The no. of Key & Value heads [Grouped-Query Attention (GQA)]\n",
    "head_dim = hidden_size // num_attention_heads   # Dimension of each atention head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31279561",
   "metadata": {},
   "source": [
    "#### Positional Embedding Parameters\n",
    "\n",
    "Instead of traditional positional embeddings, this model uses RoPE to encode the order of tokens. RoPE modifies the Query and Key vectors using rotations, which elegantly injects relative positional information directly into the self-attention calculation.  \n",
    "\n",
    "\n",
    "$\\underline{\\text{Rotary Positional Encodings (RoPE)}}$  \n",
    "* **Relative Position:** The attention score between two tokens becomes sensitive to their relative distance, not their absolute positions.\n",
    "* **No Trainable Parameters:** Positional information is added via a deterministic function, requiring no extra parameters to be learned.\n",
    "* **Long Sequence Extrapolation:** RoPE has been shown to be effective at handling sequences longer than the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3aea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positional_embeddings = 256  # max no. of positions to be calculated by RoPE\\\n",
    "rope_theta = 10000  # base for the formula to calculate frequencies for RoPE, controlling the timescale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d68c14",
   "metadata": {},
   "source": [
    "#### Normalization and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a3100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm_eps = 1e-5 # to normalise the vector embeddings\n",
    "attention_bias = 0  # 0 to keep it as a Linear Layer without an extra bias vector\n",
    "attention_dropout = 0  # Dropout probability for attention weights to prevent overfitting, for simplicity, we won't use that\n",
    "use_qk_norm = True  # To apply L2 normalization on Q & K before attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe510f59",
   "metadata": {},
   "source": [
    "#### Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d3845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d9a42b1a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df662603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # two independent sequences of text\n",
    "sequence_length = 10  # length of each sequence\n",
    "\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)  # creating sample input token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321974b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5043, -0.4161, -0.1364,  ..., -1.5856, -0.4089, -2.8163],\n",
      "         [ 1.0667, -0.0923,  0.3463,  ...,  0.5123,  1.9678, -1.6733],\n",
      "         [ 1.2775,  0.2651, -0.5682,  ..., -0.2129, -1.4258, -1.2878],\n",
      "         ...,\n",
      "         [ 1.4049, -0.0547, -0.4749,  ...,  2.6301, -0.4774,  0.3909],\n",
      "         [-0.5966,  0.7187, -0.3401,  ..., -0.5780,  0.9983,  0.6903],\n",
      "         [-0.4571,  0.7204,  0.3816,  ...,  1.9020, -0.6863,  0.4856]],\n",
      "\n",
      "        [[-1.8869,  2.0450, -0.3714,  ..., -0.0561,  1.2780, -0.0363],\n",
      "         [ 0.2985,  1.5429,  1.3085,  ...,  0.2492,  0.6134,  0.5383],\n",
      "         [-0.2063, -2.8666, -1.4368,  ..., -0.6156, -0.6485,  0.1808],\n",
      "         ...,\n",
      "         [-0.2064,  2.1962,  1.2381,  ...,  1.1080, -0.6104,  0.7092],\n",
      "         [-1.1046, -0.1936,  0.0943,  ..., -0.0681,  0.0745,  1.0041],\n",
      "         [ 0.8959,  0.1819,  1.3658,  ...,  0.7530, -0.9845, -0.2993]]])\n",
      "torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3166683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create positional ids for the tokens (very imp for RoPE)\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4ddbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.arange(0, sequence_length) : \n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "torch.arange(0, sequence_length).unsqueeze(0) : \n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "\n",
      "torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) : \n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# understanding what goes on in each step\n",
    "print(f'torch.arange(0, sequence_length) : \\n {torch.arange(0, sequence_length)}')  # creates a simple indexing sequence of numbers\n",
    "print()\n",
    "print(f'torch.arange(0, sequence_length).unsqueeze(0) : \\n {torch.arange(0, sequence_length).unsqueeze(0)}')  # Adds a new dimension of size 1 at the specified position (0-row, 1-column)\n",
    "print()\n",
    "print(f'torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) : \\n {torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)}')  # copies the sequence for each item in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65335a4a",
   "metadata": {},
   "source": [
    "#### Attention Mask\n",
    "To prevent the model from attending to the tokens after the current token, it should be able to see only the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813b8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "# create a square and assign -inf to all the upper triangle positions so the softmax functino will make it 0\n",
    "# diagonal = 1 specifies that the digonal right above the principal diagonal\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7860c7",
   "metadata": {},
   "source": [
    "* `.unsqueeze(0)` changes the dimension from `[sequence_length, sequence_length]` to `[1, sequence_length, sequence_length]`  \n",
    "\n",
    "* second `.unsqueeze(0)` changes the dimension from `[1, sequence_length, sequence_length]` to `[1, 1, sequence_length, sequence_length]`   \n",
    "\n",
    "We do this to match the dimensions with the attention_weights, which has a 4D shape `[batch_size, num_attention_heads, sequence_length, sequence_length]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e61acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attention_mask =attention_mask.unsqueeze(0).unsqueeze(0)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5108108",
   "metadata": {},
   "source": [
    "* `1` indicates that the size of the second dimension is 1. We apply the same attention mask across all attention heads. \n",
    "* `-1` indicates to not change the third and fourth dimensions, to remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb2fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7a18b",
   "metadata": {},
   "source": [
    "### Final Config Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f6e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration\n",
      "hidden_size: 128\n",
      "num_attention_heads: 16\n",
      "num_key_value_heads: 4\n",
      "head_dim: 8\n",
      "\n",
      "Sample Input Shapes\n",
      "hidden_states: torch.Size([2, 10, 128])\n",
      "position_ids: torch.Size([2, 10])\n",
      "attention_mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print('Configuration')\n",
    "print(f'hidden_size: {hidden_size}')\n",
    "print(f'num_attention_heads: {num_attention_heads}')\n",
    "print(f'num_key_value_heads: {num_key_value_heads}')\n",
    "print(f'head_dim: {head_dim}')\n",
    "print()\n",
    "print('Sample Input Shapes')\n",
    "print(f'hidden_states: {hidden_states.shape}')\n",
    "print(f'position_ids: {position_ids.shape}')\n",
    "print(f'attention_mask: {attention_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fc5f9",
   "metadata": {},
   "source": [
    "# Q, K, V Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595df0b",
   "metadata": {},
   "source": [
    "#### Define Projection Layers\n",
    "here, we define the matrices W<sup>q</sup>, W<sup>k</sup>, and W<sup>v</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df232e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias= attention_bias)\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias= attention_bias)\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias= attention_bias)\n",
    "\n",
    "# contains the learned weight matrix often refferd to as W_o on paper\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias= attention_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148a6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection layers\n",
      "q_proj: Linear(in_features=128, out_features=128, bias=False)\n",
      "k_proj: Linear(in_features=128, out_features=32, bias=False)\n",
      "v_proj: Linear(in_features=128, out_features=32, bias=False)\n",
      "o_proj: Linear(in_features=128, out_features=128, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print('Projection layers')\n",
    "print(f'q_proj: {q_proj}')\n",
    "print(f'k_proj: {k_proj}')\n",
    "print(f'v_proj: {v_proj}')\n",
    "print(f'o_proj: {o_proj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade05df",
   "metadata": {},
   "source": [
    "#### Project the input on these matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c44d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = q_proj(hidden_states)\n",
    "key_states = k_proj(hidden_states)\n",
    "value_states = v_proj(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359692ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections shape\n",
      "query_states: torch.Size([2, 10, 128])\n",
      "key_states: torch.Size([2, 10, 32])\n",
      "value_states: torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "print('Projections shape')\n",
    "print(f'query_states: {query_states.shape}')\n",
    "print(f'key_states: {key_states.shape}')\n",
    "print(f'value_states: {value_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497963f",
   "metadata": {},
   "source": [
    "* Query: We have 2 sequences, 10 tokens in each sequence, and 128 values to represent each single token.  \n",
    "* Key & Value: We have 2 sequences, 10 tokens in each sequence, and 32 values to represent each single token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bac383",
   "metadata": {},
   "source": [
    "#### Creating Individual Heads\n",
    "We have have Q, K, and V. We must divide them into individual heads for multi-head attention.  \n",
    "\n",
    "Target shape: [batch_size, num_heads, sequence_length, head_size]\n",
    "\n",
    "`view()` function is used to reshape the tensor *(works only on contiguous tensors)*  \n",
    "*we used it to split `hidden_size` dimension into two new dimensions `(num_attention_heads, head_dim)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7cf0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1,2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1,2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d1abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individual heads shapes:\n",
      "query_states: torch.Size([2, 16, 10, 8])\n",
      "key_states: torch.Size([2, 4, 10, 8])\n",
      "value_states: torch.Size([2, 4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print('The individual heads shapes:')\n",
    "print(f'query_states: {query_states.shape}')\n",
    "print(f'key_states: {key_states.shape}')\n",
    "print(f'value_states: {value_states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f6e9a",
   "metadata": {},
   "source": [
    "#### Calculating the number of Query heads per Key-Value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c67458ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Key-Value groups (Q heads per K-V head): 4\n"
     ]
    }
   ],
   "source": [
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f'Number of Key-Value groups (Q heads per K-V head): {num_key_value_groups}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94d147",
   "metadata": {},
   "source": [
    "#### Rotary Positional Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270891f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
