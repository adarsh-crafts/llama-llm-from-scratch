{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8f8399",
   "metadata": {},
   "source": [
    "## Training Corpus\n",
    "We first create our corpus (aka Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef42b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bfc83",
   "metadata": {},
   "source": [
    "## Initial Vocabulary\n",
    "Now we must create the initial vocabulary which will have our unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616391a1",
   "metadata": {},
   "source": [
    "create a list of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5a6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (no duplicates are allowed in sets)\n",
    "unique_chars = set()\n",
    "\n",
    "# add chars from corpus to set\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc401e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.', 'o', 'r', 'T', 's', 'n', 'i', 'u', 'f', 'e', 'h', 'A', 'c', ' ', '?', 't', 'd', 'I', 'm'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ed1fd",
   "metadata": {},
   "source": [
    "we now convert it into a list.\n",
    "(sets are immutable and cannot be indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd12617",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(unique_chars)\n",
    "vocab.sort()        # simply, for coninstency and repoducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5595f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf6d8f",
   "metadata": {},
   "source": [
    "add an **end of word** token.  \n",
    "> so the model will be able to differentiate between words and avoid irrelevant/wrong pairs of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58e145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_word = '/<w>'\n",
    "vocab.append(end_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7d16a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>']\n",
      "size: 20\n"
     ]
    }
   ],
   "source": [
    "print('Initial Vocabulary:')\n",
    "print(vocab)\n",
    "print(f'size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e96b91",
   "metadata": {},
   "source": [
    "## Pre-Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320638ef",
   "metadata": {},
   "source": [
    "here, we will split the corpus into words, then characters.\n",
    "- to split into words, we'll use the space character\n",
    "- we will add `</w>>` at the end of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1139610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The final dictionary with word count:\n",
      "{('T', 'h', 'i', 's', '/<w>'): 2, ('i', 's', '/<w>'): 3, ('t', 'h', 'e', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('t', 'h', 'i', 's', '/<w>'): 2, ('t', 'h', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's', '/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "word_splits = {}\n",
    "\n",
    "for doc in corpus:\n",
    "\n",
    "    # splitting by ' ' character\n",
    "    words = doc.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        \n",
    "        char_list = list(word) + [end_of_word]      # convert words into list and append the char\n",
    "\n",
    "        # convert to list because we will need an immutable object to act as a key in the dictionary\n",
    "        word_tuple = tuple(char_list)\n",
    "\n",
    "        if word_tuple not in word_splits:\n",
    "            word_splits[word_tuple] = 0\n",
    "        word_splits[word_tuple] += 1                # incrememnting count for each word when found\n",
    "\n",
    "print('\\nThe final dictionary with word count:')\n",
    "print(word_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94a9c8",
   "metadata": {},
   "source": [
    "## Helper Functions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea90f38",
   "metadata": {},
   "source": [
    "### get_pair_stats()\n",
    "This function will pair the adjecent characters and count their frequency.  \n",
    "example:  \n",
    "**input** =  \n",
    "``` {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...} ```\n",
    "  \n",
    "**output** =  \n",
    "``` # {('i', 's'): 4, ('s', '</w>'): 4, ('T', 'h'): 2, ...} ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66906a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    # A collection's dictionary will create a new key if it already doesn't exist in the dictionary.\n",
    "    pair_counts = collections.defaultdict(int)      #defaultdict will have default values of 0\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)                  # converting tuple to list\n",
    "\n",
    "        for i in range(len(symbols)-1):               # iterating through each element in the word\n",
    "            pair = (symbols[i], symbols[i+1])       # pairing chars with the next char\n",
    "            pair_counts[pair] += freq               # addin the frequency of the word\n",
    "    \n",
    "    return pair_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0972df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('T', 'h'): 2, ('h', 'i'): 5, ('i', 's'): 7, ('s', '/<w>'): 8, ('t', 'h'): 7, ('h', 'e'): 4, ('e', '/<w>'): 4, ('f', 'i'): 2, ('i', 'r'): 3, ('r', 's'): 2, ('s', 't'): 2, ('t', '/<w>'): 3, ('d', 'o'): 4, ('o', 'c'): 4, ('c', 'u'): 4, ('u', 'm'): 4, ('m', 'e'): 4, ('e', 'n'): 4, ('n', 't'): 4, ('t', '.'): 2, ('.', '/<w>'): 3, ('s', 'e'): 1, ('e', 'c'): 1, ('c', 'o'): 1, ('o', 'n'): 2, ('n', 'd'): 2, ('d', '/<w>'): 3, ('A', 'n'): 1, ('r', 'd'): 1, ('n', 'e'): 1, ('e', '.'): 1, ('I', 's'): 1, ('t', '?'): 1, ('?', '/<w>'): 1})\n"
     ]
    }
   ],
   "source": [
    "pair_counts_dict = get_pair_stats(word_splits)\n",
    "print(pair_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f896e",
   "metadata": {},
   "source": [
    "Now we have the frequency of occurence of each pair of characters in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead80aea",
   "metadata": {},
   "source": [
    "### merge_pair()\n",
    "\n",
    "* Now we will merge the pairs with the most frequencies into a new token.  \n",
    "* Addionally, we will keep a track of these merges so we can undo everything when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair_to_merge, splits):\n",
    "    new_splits = {}\n",
    "    (first_char, second_char) = pair_to_merge\n",
    "    merged_token = first_char + second_char\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        new_symbols = []\n",
    "\n",
    "        i=0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == first_char and symbols[i+1] == second_char:\n",
    "                new_symbols.append(merged_token)\n",
    "                i+=2        # to skip to the next symbol\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i+=1\n",
    "        new_splits[tuple(new_symbols)] = freq\n",
    "    return new_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27d6ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('T', 'h'): 2,\n",
       " ('h', 'i'): 5,\n",
       " ('is',): 7,\n",
       " ('s', '/<w>'): 8,\n",
       " ('t', 'h'): 7,\n",
       " ('h', 'e'): 4,\n",
       " ('e', '/<w>'): 4,\n",
       " ('f', 'i'): 2,\n",
       " ('i', 'r'): 3,\n",
       " ('r', 's'): 2,\n",
       " ('s', 't'): 2,\n",
       " ('t', '/<w>'): 3,\n",
       " ('d', 'o'): 4,\n",
       " ('o', 'c'): 4,\n",
       " ('c', 'u'): 4,\n",
       " ('u', 'm'): 4,\n",
       " ('m', 'e'): 4,\n",
       " ('e', 'n'): 4,\n",
       " ('n', 't'): 4,\n",
       " ('t', '.'): 2,\n",
       " ('.', '/<w>'): 3,\n",
       " ('s', 'e'): 1,\n",
       " ('e', 'c'): 1,\n",
       " ('c', 'o'): 1,\n",
       " ('o', 'n'): 2,\n",
       " ('n', 'd'): 2,\n",
       " ('d', '/<w>'): 3,\n",
       " ('A', 'n'): 1,\n",
       " ('r', 'd'): 1,\n",
       " ('n', 'e'): 1,\n",
       " ('e', '.'): 1,\n",
       " ('I', 's'): 1,\n",
       " ('t', '?'): 1,\n",
       " ('?', '/<w>'): 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_pair(('i','s'), pair_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b9e7a",
   "metadata": {},
   "source": [
    "The `merge_pair()` function has created a new \"is\" token and it appears 7 times in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd88d8f",
   "metadata": {},
   "source": [
    "# BPE Merging Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070d3be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BPE Merges\n",
      "Initial Splits{('T', 'h', 'i', 's', '/<w>'): 2, ('i', 's', '/<w>'): 3, ('t', 'h', 'e', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('t', 'h', 'i', 's', '/<w>'): 2, ('t', 'h', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's', '/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_merges = 15\n",
    "merges = {}\n",
    "current_splits = word_splits.copy()\n",
    "\n",
    "print('Starting BPE Merges')\n",
    "print(f'Initial Splits{current_splits}')\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8da36472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge iteration 1/15\n",
      "Found Best Pair: ('s', '/<w>') with Frequency 8\n",
      "Merging ('s', '/<w>') into `s/<w>`\n",
      "splits after merge: {('T', 'h', 'i', 's/<w>'): 2, ('i', 's/<w>'): 3, ('t', 'h', 'e', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('t', 'h', 'i', 's/<w>'): 2, ('t', 'h', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>'}\n",
      "------------------------------\n",
      "Merge iteration 2/15\n",
      "Found Best Pair: ('i', 's/<w>') with Frequency 7\n",
      "Merging ('i', 's/<w>') into `is/<w>`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('t', 'h', 'e', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('t', 'h', 'is/<w>'): 2, ('t', 'h', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>'}\n",
      "------------------------------\n",
      "Merge iteration 3/15\n",
      "Found Best Pair: ('t', 'h') with Frequency 7\n",
      "Merging ('t', 'h') into `th`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('th', 'e', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th'}\n",
      "------------------------------\n",
      "Merge iteration 4/15\n",
      "Found Best Pair: ('th', 'e') with Frequency 4\n",
      "Merging ('th', 'e') into `the`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the'}\n",
      "------------------------------\n",
      "Merge iteration 5/15\n",
      "Found Best Pair: ('the', '/<w>') with Frequency 4\n",
      "Merging ('the', '/<w>') into `the/<w>`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>'}\n",
      "------------------------------\n",
      "Merge iteration 6/15\n",
      "Found Best Pair: ('d', 'o') with Frequency 4\n",
      "Merging ('d', 'o') into `do`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('do', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do'}\n",
      "------------------------------\n",
      "Merge iteration 7/15\n",
      "Found Best Pair: ('do', 'c') with Frequency 4\n",
      "Merging ('do', 'c') into `doc`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('doc', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc'}\n",
      "------------------------------\n",
      "Merge iteration 8/15\n",
      "Found Best Pair: ('doc', 'u') with Frequency 4\n",
      "Merging ('doc', 'u') into `docu`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('docu', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('docu', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('docu', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu'}\n",
      "------------------------------\n",
      "Merge iteration 9/15\n",
      "Found Best Pair: ('docu', 'm') with Frequency 4\n",
      "Merging ('docu', 'm') into `docum`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('docum', 'e', 'n', 't', '.', '/<w>'): 2, ('docum', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('docum', 'e', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum'}\n",
      "------------------------------\n",
      "Merge iteration 10/15\n",
      "Found Best Pair: ('docum', 'e') with Frequency 4\n",
      "Merging ('docum', 'e') into `docume`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('docume', 'n', 't', '.', '/<w>'): 2, ('docume', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('docume', 'n', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum', 'docume']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume'}\n",
      "------------------------------\n",
      "Merge iteration 11/15\n",
      "Found Best Pair: ('docume', 'n') with Frequency 4\n",
      "Merging ('docume', 'n') into `documen`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('documen', 't', '.', '/<w>'): 2, ('documen', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('documen', 't', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen'}\n",
      "------------------------------\n",
      "Merge iteration 12/15\n",
      "Found Best Pair: ('documen', 't') with Frequency 4\n",
      "Merging ('documen', 't') into `document`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('document', '.', '/<w>'): 2, ('document', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('document', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document'}\n",
      "------------------------------\n",
      "Merge iteration 13/15\n",
      "Found Best Pair: ('i', 'r') with Frequency 3\n",
      "Merging ('i', 'r') into `ir`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'ir', 's', 't', '/<w>'): 2, ('document', '.', '/<w>'): 2, ('document', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'ir', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's/<w>'): 1, ('document', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir'}\n",
      "------------------------------\n",
      "Merge iteration 14/15\n",
      "Found Best Pair: ('.', '/<w>') with Frequency 3\n",
      "Merging ('.', '/<w>') into `./<w>`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'ir', 's', 't', '/<w>'): 2, ('document', './<w>'): 2, ('document', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'ir', 'd', '/<w>'): 1, ('o', 'n', 'e', './<w>'): 1, ('I', 's/<w>'): 1, ('document', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', './<w>']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '/<w>'): './<w>'}\n",
      "------------------------------\n",
      "Merge iteration 15/15\n",
      "Found Best Pair: ('d', '/<w>') with Frequency 3\n",
      "Merging ('d', '/<w>') into `d/<w>`\n",
      "splits after merge: {('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'ir', 's', 't', '/<w>'): 2, ('document', './<w>'): 2, ('document', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd/<w>'): 1, ('A', 'n', 'd/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'ir', 'd/<w>'): 1, ('o', 'n', 'e', './<w>'): 1, ('I', 's/<w>'): 1, ('document', '?', '/<w>'): 1}\n",
      "Updated Vocab: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>', 's/<w>', 'is/<w>', 'th', 'the', 'the/<w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', './<w>', 'd/<w>']\n",
      "Updated Merges: {('s', '/<w>'): 's/<w>', ('i', 's/<w>'): 'is/<w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '/<w>'): 'the/<w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '/<w>'): './<w>', ('d', '/<w>'): 'd/<w>'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_merges):\n",
    "    print(f'Merge iteration {i+1}/{num_merges}')\n",
    "\n",
    "    # calculate pair frequencies\n",
    "    pair_stats = get_pair_stats(current_splits)\n",
    "    \n",
    "    if not pair_stats:\n",
    "        print('No more pairs to merge.')\n",
    "        break\n",
    "\n",
    "    # find best pair\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)     # so that it will check for max frequency instead of the key value\n",
    "    best_freq = pair_stats[best_pair]\n",
    "    print(f'Found Best Pair: {best_pair} with Frequency {best_freq}')\n",
    "\n",
    "    # merge the best pair\n",
    "    current_splits = merge_pair(best_pair, current_splits)\n",
    "    new_token = best_pair[0] + best_pair[1]\n",
    "    print(f'Merging {best_pair} into `{new_token}`')\n",
    "    print(f'splits after merge: {current_splits}')\n",
    "\n",
    "    # update vocab\n",
    "    vocab.append(new_token)\n",
    "    print(f'Updated Vocab: {vocab}')\n",
    "\n",
    "    # store the merge\n",
    "    merges[best_pair] = new_token\n",
    "    print(f'Updated Merges: {merges}')\n",
    "\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ec4ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BPE Merges Complete ---\n",
      "Final Vocabulary Size: 35\n",
      "\n",
      "Learned Merges (Pair -> New Token):\n",
      "('s', '/<w>') -> 's/<w>'\n",
      "('i', 's/<w>') -> 'is/<w>'\n",
      "('t', 'h') -> 'th'\n",
      "('th', 'e') -> 'the'\n",
      "('the', '/<w>') -> 'the/<w>'\n",
      "('d', 'o') -> 'do'\n",
      "('do', 'c') -> 'doc'\n",
      "('doc', 'u') -> 'docu'\n",
      "('docu', 'm') -> 'docum'\n",
      "('docum', 'e') -> 'docume'\n",
      "('docume', 'n') -> 'documen'\n",
      "('documen', 't') -> 'document'\n",
      "('i', 'r') -> 'ir'\n",
      "('.', '/<w>') -> './<w>'\n",
      "('d', '/<w>') -> 'd/<w>'\n",
      "\n",
      "Final Word Splits after all merges:\n",
      "{('T', 'h', 'is/<w>'): 2, ('is/<w>',): 3, ('the/<w>',): 4, ('f', 'ir', 's', 't', '/<w>'): 2, ('document', './<w>'): 2, ('document', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd/<w>'): 1, ('A', 'n', 'd/<w>'): 1, ('th', 'is/<w>'): 2, ('th', 'ir', 'd/<w>'): 1, ('o', 'n', 'e', './<w>'): 1, ('I', 's/<w>'): 1, ('document', '?', '/<w>'): 1}\n",
      "\n",
      "Final Vocabulary (sorted):\n",
      "[' ', '.', './<w>', '/<w>', '?', 'A', 'I', 'T', 'c', 'd', 'd/<w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'e', 'f', 'h', 'i', 'ir', 'is/<w>', 'm', 'n', 'o', 'r', 's', 's/<w>', 't', 'th', 'the', 'the/<w>', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- BPE Merges Complete ---\")\n",
    "print(f\"Final Vocabulary Size: {len(vocab)}\")\n",
    "print(\"\\nLearned Merges (Pair -> New Token):\")\n",
    "# Pretty print merges\n",
    "for pair, token in merges.items():\n",
    "    print(f\"{pair} -> '{token}'\")\n",
    "\n",
    "print(\"\\nFinal Word Splits after all merges:\")\n",
    "print(current_splits)\n",
    "\n",
    "print(\"\\nFinal Vocabulary (sorted):\")\n",
    "# Sort for consistent viewing\n",
    "final_vocab_sorted = sorted(list(set(vocab))) # Use set to remove potential duplicates if any step introduced them\n",
    "print(final_vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5a826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
