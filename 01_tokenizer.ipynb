{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8f8399",
   "metadata": {},
   "source": [
    "## Training Corpus\n",
    "We first create our corpus (aka Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef42b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bfc83",
   "metadata": {},
   "source": [
    "## Initial Vocabulary\n",
    "Now we must create the initial vocabulary which will have our unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616391a1",
   "metadata": {},
   "source": [
    "create a list of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5a6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (no duplicates are allowed in sets)\n",
    "unique_chars = set()\n",
    "\n",
    "# add chars from corpus to set\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc401e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A', '?', ' ', '.', 'u', 't', 's', 'm', 'f', 'I', 'd', 'n', 'T', 'c', 'h', 'i', 'o', 'e', 'r'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ed1fd",
   "metadata": {},
   "source": [
    "we now convert it into a list.\n",
    "(sets are immutable and cannot be indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd12617",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(unique_chars)\n",
    "vocab.sort()        # simply, for coninstency and repoducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5595f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf6d8f",
   "metadata": {},
   "source": [
    "add an **end of word** token.  \n",
    "> so the model will be able to differentiate between words and avoid irrelevant/wrong pairs of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58e145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_word = '/<w>'\n",
    "vocab.append(end_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7d16a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '/<w>']\n",
      "size: 20\n"
     ]
    }
   ],
   "source": [
    "print('Initial Vocabulary:')\n",
    "print(vocab)\n",
    "print(f'size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e96b91",
   "metadata": {},
   "source": [
    "## Pre-Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320638ef",
   "metadata": {},
   "source": [
    "here, we will split the corpus into words, then characters.\n",
    "- to split into words, we'll use the space character\n",
    "- we will add `</w>>` at the end of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The final dictionary with word count:\n",
      "{('T', 'h', 'i', 's', '/<w>'): 2, ('i', 's', '/<w>'): 3, ('t', 'h', 'e', '/<w>'): 4, ('f', 'i', 'r', 's', 't', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '/<w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '/<w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '/<w>'): 1, ('A', 'n', 'd', '/<w>'): 1, ('t', 'h', 'i', 's', '/<w>'): 2, ('t', 'h', 'i', 'r', 'd', '/<w>'): 1, ('o', 'n', 'e', '.', '/<w>'): 1, ('I', 's', '/<w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "word_splits = {}\n",
    "\n",
    "for doc in corpus:\n",
    "\n",
    "    # splitting by ' ' character\n",
    "    words = doc.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        \n",
    "        char_list = list(word) + [end_of_word]      # convert words into list and append the char\n",
    "\n",
    "        # convert to list because we will need an immutable object to act as a key in the dictionary\n",
    "        word_tuple = tuple(char_list)\n",
    "\n",
    "        if word_tuple not in word_splits:\n",
    "            word_splits[word_tuple] = 0\n",
    "        word_splits[word_tuple] += 1                # incrememnting count for each word when found\n",
    "\n",
    "print('\\nThe final dictionary with word count:')\n",
    "print(word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba84cbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '/<w>')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea90f38",
   "metadata": {},
   "source": [
    "## Helper Function: get_pair_stats  \n",
    "This function will pair the adjecent characters and count their frequency.  \n",
    "example:  \n",
    "**input** =  \n",
    "``` {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...} ```\n",
    "  \n",
    "**output** =  \n",
    "``` # {('i', 's'): 4, ('s', '</w>'): 4, ('T', 'h'): 2, ...} ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66906a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    # A collection's dictionary will create a new key if it already doesn't exist in the dictionary.\n",
    "    pair_counts = collections.defaultdict(int)      #defaultdict will have default values of 0\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)                  # converting tuple to list\n",
    "\n",
    "        for i in range(len(symbols)):               # iterating through each element in the word\n",
    "            pair = (symbols[i], symbols[i+1])       # pairing chars with the next char\n",
    "            pair_counts[pair] += freq               # addin the frequency of the word\n",
    "    \n",
    "    return pair_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d6ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
